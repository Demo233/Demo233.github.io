<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>YIHAO&#39;S BLOG</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="yihao.ml/"/>
  <updated>2019-07-02T02:01:03.772Z</updated>
  <id>yihao.ml/</id>
  
  <author>
    <name>Yihao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>时间空间复杂度</title>
    <link href="yihao.ml/2019/07/01/2019-07-01-%E6%97%B6%E9%97%B4%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/"/>
    <id>yihao.ml/2019/07/01/2019-07-01-时间空间复杂度/</id>
    <published>2019-07-01T11:15:05.000Z</published>
    <updated>2019-07-02T02:01:03.772Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>算法与数据结构相辅相成，谁也离不开谁，学算法和数据结构可以说让我很痛苦，当然对于聪明的你来说，这可能不是什么难事,哈哈哈。这下面的图是算法的整个知识体系图，通过让我们对整个算法体系有个初步了解。</p><p><a href="https://i.loli.net/2019/07/01/5d197b9bcad7383009.jpg" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/07/01/5d197b9bcad7383009.jpg" alt="sf_1.jpg"></a></p><p>我总结了20个最常用的、最基础数据结构与算法,他们分别是10个数据结构：<font color="red">数组、</font><font color="red">链表、</font><font color="red">栈、</font><font color="red">队列、</font><font color="red">散列表、</font><font color="red">二叉树、</font><font color="red">堆、</font><font color="red">跳表、</font><font color="red">图、</font><font color="red">Trie树；10个算法：<font color="red">递归、</font><font color="red">排序、</font><font color="red">二分查找、</font><font color="red">搜索、</font><font color="red">哈希算法、</font><font color="red">贪心算法、</font><font color="red">分治算法、</font><font color="red">回溯算法、</font><font color="red">动态规划、</font><font color="red">字符串匹配</font>算法。</font></p><p>如果想学好算法，就得掌握怎么分析代码的性能。业界中常用时间复杂度和空间复杂度来坑量，类似O(1)、O(n)、O(n^2)等。如果说不会求时间和空间复杂度，那只能说你和算法无缘了！下面我们来理理到底应该怎么算，另外算这些东西不需要什么很高深的数学，只需要高中数学知识就足够了。下面会牵扯到一些对数运算，如果你忘了，建议你回去复习一下。</p><h3 id="大o复杂度表示法"><a class="markdownIt-Anchor" href="#大o复杂度表示法"></a> 大O复杂度表示法</h3><p>这里有段非常简单的代码，求1,2,3…n的累加和。现在，我就带你一块来估算一下这段代码的执行时间。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cal</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; i &lt;= n; ++i) &#123;</span><br><span class="line">sum = sum + i;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>假设每行代码执行的时间都一样，为unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？</p><p>第2、3行代码分别需要1个unit_time的执行时间，第4、5行都运行了n遍，所以需要2n * unit_time的执行时间，所以这段代码总的执行时间就是(2n+2) * unit_time。可以看出来，<strong>所有代码的执行时间T(n)与每行代码的执行次数成正比</strong></p><p>按照这个分析思路，我们再来看这段代码。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cal</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> j = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; i &lt;= n; ++i) &#123;</span><br><span class="line">j = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; j &lt;= n; ++j) &#123;</span><br><span class="line">sum = sum +  i * j;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们依旧假设每个语句的执行时间是unit_time。那这段代码的总执行时间T(n)是多少呢？</p><p>第2、3、4行代码，每行都需要1个unit_time的执行时间，第5、6行代码循环执行了n遍，需要2n * unit_time的执行时间，第7、8行代码循环执行了n^2 遍，所以需要 2n^2 * unit_time的执行时间。所以，整段代码总的执行时间T(n) = (2n ^ 2 + 2n+3)*unit_time</p><p>尽管我们不知道unit_time的具体值，但是通过这两段代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，<font color="red">所有代码的执行时间T(n)与每行代码的执行次数n成正比</font>。</p><p>我们可以把这个规律总结成一个公式。注意，大O就要登场了！</p><p><a href="https://i.loli.net/2019/07/01/5d1990ab0c2ec77731.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/07/01/5d1990ab0c2ec77731.png" alt="sf_2.png"></a></p><p>我来具体解释一下这个公式。其中，T(n)我们已经讲过了，它表示代码执行的时间；n表示数据规模的大小；f(n)表示每行代码执行的次数总和。因为这是一个公式，所以用f(n)来表示。公式中的O，表示代码的执行时间T(n)与f(n)表达式成正比。</p><p>所以，第一个例子中的T(n) = O(2n+2)，第二个例子中的T(n) = O(2n^2+2n+3)。这就是<strong>大O时间复杂度表示法</strong>。大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示<strong>代码执行时间随数据规模增长的变化趋势</strong>，所以，也叫作<strong>渐进时间复杂度</strong>（asymptotic time complexity），简称<strong>时间复杂度</strong>。</p><p>当n很大时，你可以把它想象成10000、100000。而公式中的<font color="red">低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略</font>。我们只需要记录一个最大量级就可以了，如果用大O表示法表示刚讲的那两段代码的时间复杂度，就可以记为：T(n) = O(n)； T(n) = O(n^2)。</p><h3 id="时间复杂度分析"><a class="markdownIt-Anchor" href="#时间复杂度分析"></a> 时间复杂度分析</h3><p>前面介绍了大O时间复杂度的由来和表示方法。现在我们来看下，如何分析一段代码的时间复杂度？我这儿有三个比较实用的方法可以分享给你。</p><h4 id="1只关注循环执行次数最多的一段代码"><a class="markdownIt-Anchor" href="#1只关注循环执行次数最多的一段代码"></a> 1.只关注循环执行次数最多的一段代码</h4><p>我刚才说了，大O这种复杂度表示方法只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。所以，<strong>我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了</strong>。这段核心代码执行次数的n的量级，就是整段要分析代码的时间复杂度。</p><p>为了便于你理解，我还拿前面的例子来说明。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cal</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; i &lt;= n; ++i) &#123;</span><br><span class="line">sum = sum + i;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中第2、3行代码都是常量级的执行时间，与n的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第4、5行代码，所以这块代码要重点分析。前面我们也讲过，这两行代码被执行了n次，所以总的时间复杂度就是O(n)。</p><h4 id="2加法法则总复杂度等于量级最大的那段代码的复杂度"><a class="markdownIt-Anchor" href="#2加法法则总复杂度等于量级最大的那段代码的复杂度"></a> 2.加法法则：总复杂度等于量级最大的那段代码的复杂度</h4><p>我这里还有一段代码。你可以先试着分析一下，然后再往下看跟我的分析思路是否一样</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cal</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> sum_1 = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> p = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; p &lt; <span class="number">100</span>; ++p) &#123;</span><br><span class="line">sum_1 = sum_1 + p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> sum_2 = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> q = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; q &lt; n; ++q) &#123;</span><br><span class="line">sum_2 = sum_2 + q;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> sum_3 = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> j = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; i &lt;= n; ++i) &#123;</span><br><span class="line">j = <span class="number">1</span>; </span><br><span class="line"><span class="keyword">for</span> (; j &lt;= n; ++j) &#123;</span><br><span class="line">sum_3 = sum_3 +  i * j;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> sum_1 + sum_2 + sum_3;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个代码分为三部分，分别是求sum_1、sum_2、sum_3。我们可以分别分析每一部分的时间复杂度，然后把它们放到一块儿，再取一个量级最大的作为整段代码的复杂度。</p><p>第一段的时间复杂度是多少呢？这段代码循环执行了100次，所以是一个常量的执行时间，跟n的规模无关。</p><p>这里我要再强调一下，即便这段代码循环10000次、100000次，只要是一个已知的数，跟n无关，照样也是常量级的执行时间。当n无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。</p><p>那第二段代码和第三段代码的时间复杂度是多少呢？答案是O(n)和O(n^2)，你应该能容易就分析出来，我就不啰嗦了。</p><p>综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就为O(n^2)。也就是说：<strong>总的时间复杂度就等于量级最大的那段代码的时间复杂度</strong>。那我们将这个规律抽象成公式就是：</p><p>如果T1(n)=O(f(n))，T2(n)=O(g(n))；那么T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n))).</p><h4 id="3乘法法则嵌套代码的复杂度等于嵌套内外代码复杂度的乘积"><a class="markdownIt-Anchor" href="#3乘法法则嵌套代码的复杂度等于嵌套内外代码复杂度的乘积"></a> 3.乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积</h4><p>我刚讲了一个复杂度分析中的加法法则，这儿还有一个<strong>乘法法则</strong>。类比一下，你应该能“猜到”公式是什么样子的吧？</p><p>如果T1(n)=O(f(n))，T2(n)=O(g(n))；那么T(n)=T1(n) * T2(n) = O(f(n)) * O(g(n)) = O(f(n) * g(n)).</p><p>也就是说，假设T1(n) = O(n)，T2(n) = O(n^2)，则T1(n) * T2(n) = O(n^3)。落实到具体的代码上，我们可以把乘法法则看成是<strong>嵌套循环</strong>，我举个例子给你解释一下。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cal</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> ret = <span class="number">0</span>; </span><br><span class="line"><span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; i &lt; n; ++i) &#123;</span><br><span class="line">ret = ret + f(i);</span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; i &lt; n; ++i) &#123;</span><br><span class="line">sum = sum + i;</span><br><span class="line">&#125; </span><br><span class="line"><span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们单独看cal()函数。假设f()只是一个普通的操作，那第4～6行的时间复杂度就是，T1(n) = O(n)。但f()函数本身不是一个简单的操作，它的时间复杂度是T2(n) = O(n)，所以，整个cal()函数的时间复杂度就是，T(n) = T1(n) * T2(n) = O(n*n) = O(n^2)。</p><p>我刚刚讲了三种复杂度的分析技巧。不过，你并不用刻意去记忆。实际上，复杂度分析这个东西关键在于“熟练”。你只要多看案例，多分析，就能做到“无招胜有招”。</p><h3 id="几种常见时间复杂度实例分析"><a class="markdownIt-Anchor" href="#几种常见时间复杂度实例分析"></a> 几种常见时间复杂度实例分析</h3><p>虽然代码千差万别，但是常见的复杂度量级并不多。我稍微总结了一下，这些复杂度量级几乎涵盖了你今后可以接触的所有代码的复杂度量级。</p><p><a href="https://i.loli.net/2019/07/01/5d1992d63e0b039265.jpg" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/07/01/5d1992d63e0b039265.jpg" alt="sf_3.jpg"></a></p><p>对于刚罗列的复杂度量级，我们可以粗略地分为两类，<strong>多项式量级</strong>和<strong>非多项式量级</strong>。其中，非多项式量级只有两个：O(2^n)和O(n!)。</p><p>当数据规模n越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。因此，关于NP时间复杂度我就不展开讲了。我们主要来看几种常见的<strong>多项式时间复杂度</strong>。</p><h4 id="1-o1"><a class="markdownIt-Anchor" href="#1-o1"></a> 1. O(1)</h4><p>首先你必须明确一个概念，O(1)只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。比如这段代码，即便有3行，它的时间复杂度也是O(1），而不是O(3)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">8</span>;</span><br><span class="line"><span class="keyword">int</span> j = <span class="number">6</span>;</span><br><span class="line"><span class="keyword">int</span> sum = i + j;</span><br></pre></td></tr></table></figure><p>我稍微总结一下，只要代码的执行时间不随n的增大而增长，这样代码的时间复杂度我们都记作O(1)。或者说，<strong>一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)</strong>。</p><h4 id="2-ologn-onlogn"><a class="markdownIt-Anchor" href="#2-ologn-onlogn"></a> 2. O(logn)、O(nlogn)</h4><p>对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。我通过一个例子来说明一下。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">i=<span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (i &lt;= n)  &#123;</span><br><span class="line">    i = i * <span class="number">2</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>根据我们前面讲的复杂度分析方法，第三行代码是循环执行次数最多的。所以，我们只要能计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。</p><p>从代码中可以看出，变量i的值从1开始取，每循环一次就乘以2。当大于n时，循环结束。还记得我们高中学过的等比数列吗？实际上，变量i的取值就是一个等比数列。如果我把它一个一个列出来，就应该是这个样子的：</p><p><a href="https://i.loli.net/2019/07/01/5d19935fd6bd034165.jpg" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/07/01/5d19935fd6bd034165.jpg" alt="sf_4.jpg"></a></p><p>所以，我们只要知道x值是多少，就知道这行代码执行的次数了。通过2 ^ x=n求解x这个问题我们想高中应该就学过了，我就不多说了。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mi>log</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">x=\log n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mord mathit">n</span></span></span></span> <font color="red">(以2为底)</font>所以，这段代码的时间复杂度就是：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>log</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span><font color="red">(以2为底)</font></p><p>现在，把代码稍微改下，你再看看，这段代码的时间复杂度是多少？</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">i=<span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (i &lt;= n)  &#123;</span><br><span class="line">    i = i * <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>根据我刚刚讲的思路，很简单就能看出来，这段代码的时间复杂度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(logn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span> <font color="red">(以3为底)</font>。</p><p>实际上，不管是以2为底、以3为底，还是以10为底，我们可以把所有对数阶的时间复杂度都记为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(logn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span><font color="red">(以2为底)</font>。为什么呢，我知道有些人忘了，下面我帮你论证了一下，用对数的换底公式就可以了！</p><p><a href="https://i.loli.net/2019/07/01/5d19a5c4ada8311497.jpg" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/07/01/5d19a5c4ada8311497.jpg" alt="sf_6.jpg"></a></p><p>基于我们前面的一个理论：<strong>在采用大O标记复杂度的时候，可以忽略系数，即O(Cf(n)) = O(f(n))</strong>。所以，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(logn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span> <font color="red">(以2为底)</font> 就等于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(logn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span> <font color="red">(以3为底)</font>。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(logn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span>。</p><p>如果你理解了我前面讲的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(logn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span>，那<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(nlogn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span>就很容易理解了。还记得我们刚讲的乘法法则吗？如果一段代码的时间复杂度是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(logn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span>，我们循环执行n遍，时间复杂度就是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(nlogn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span>了。而且，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(nlogn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span>也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(nlogn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span>。</p><h4 id="3omn-omn"><a class="markdownIt-Anchor" href="#3omn-omn"></a> 3.O(m+n)、O(m*n)</h4><p>我们再来讲一种跟前面都不一样的时间复杂度，代码的复杂度<strong>由两个数据的规模</strong>来决定。老规矩，先看代码！</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cal</span><span class="params">(<span class="keyword">int</span> m, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> sum_1 = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; i &lt; m; ++i) &#123;</span><br><span class="line">sum_1 = sum_1 + i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> sum_2 = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> j = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (; j &lt; n; ++j) &#123;</span><br><span class="line">sum_2 = sum_2 + j;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> sum_1 + sum_2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从代码中可以看出，m和n是表示两个数据规模。我们无法事先评估m和n谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是O(m+n)。</p><p>针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))。</p><h3 id="空间复杂度分析"><a class="markdownIt-Anchor" href="#空间复杂度分析"></a> 空间复杂度分析</h3><p>前面，咱们花了很长时间讲大O表示法和时间复杂度分析，理解了前面讲的内容，空间复杂度分析方法学起来就非常简单了</p><p>前面我讲过，时间复杂度的全称是<strong>渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系</strong>。类比一下，空间复杂度全称就是<strong>渐进空间复杂度</strong>（asymptotic space complexity），<strong>表示算法的存储空间与数据规模之间的增长关系</strong>。</p><p>我还是拿具体的例子来给你说明。（这段代码有点“傻”，一般没人会这么写，我这么写只是为了方便给你解释。）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span>[] a = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line"><span class="keyword">for</span> (i; i &lt;n; ++i) &#123;</span><br><span class="line">a[i] = i * i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (i = n<span class="number">-1</span>; i &gt;= <span class="number">0</span>; --i) &#123;</span><br><span class="line">print out a[i]</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>跟时间复杂度分析一样，我们可以看到，第2行代码中，我们申请了一个空间存储变量i，但是它是常量阶的，跟数据规模n没有关系，所以我们可以忽略。第3行申请了一个大小为n的int类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是O(n)。</p><p>我们常见的空间复杂度就是O(1)、O(n)、O(n2 )，像O(logn)、O(nlogn)这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多。所以，对于空间复杂度，掌握刚我说的这些内容已经足够了。</p><h3 id="小结"><a class="markdownIt-Anchor" href="#小结"></a> 小结</h3><p>基础复杂度分析的知识到此就讲完了，我们来总结一下。</p><p>复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(logn)、O(n)、O(nlogn)、O(n2 )。等你学完整个专栏之后，你就会发现几乎所有的数据结构和算法的复杂度都跑不出这几个。</p><p><a href="https://i.loli.net/2019/07/01/5d199477cc53579735.jpg" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/07/01/5d199477cc53579735.jpg" alt="sf_5.jpg"></a></p><p>复杂度分析并不难，关键在于多练。 之后讲后面的内容时，我还会带你详细地分析每一种数据结构和算法的时间、空间复杂度。只要跟着我的思路学习、练习，你很快就能和我一样，每次看到代码的时候，简单的一眼就能看出其复杂度，难的稍微分析一下就能得出答案。</p><p>转自《数据结构与算法之美》–王争</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;算法与数据结构相辅相成，谁也离不开谁，学算法和数据结构可以说让我很痛苦，当然对于聪明的你来说，这可能不是什么难
      
    
    </summary>
    
      <category term="算法与数据结构" scheme="yihao.ml/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>Hbase读写操作</title>
    <link href="yihao.ml/2019/06/28/2019-06-28-Hbase%E8%AF%BB%E5%86%99%E6%93%8D%E4%BD%9C/"/>
    <id>yihao.ml/2019/06/28/2019-06-28-Hbase读写操作/</id>
    <published>2019-06-28T14:18:20.000Z</published>
    <updated>2019-07-02T02:01:03.771Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>HDFS、MR解决了分布式存储和分布式计算问题，但是由于HDFS的随机读写能力太差，所以不能直接作为数据库。Hbase是为了应对这点而诞生的，它是一个高性能、高可靠、可伸缩、面向列的分布式存储数据库，结合Zookeeper可以解决HDFS随机读写能力差的问题。那么它到底是怎么解决随机读写能力太差的问题呢？试想一下如果想要1S 往某个文件中插入100条记录，如果没有HBase，用Java代码写会是一种什么样的操作？可能我们需要100次的IO才能搞定。但上面假设一层缓冲层用来缓存一下， 当缓冲池满了以后再往文件中写，会不会好很多？</p><p>以Mysql为例，我们都知道Mysql的表，库等数据最终都会落在磁盘上，Mysql只不过是架设在OS文件系统上的一款解析软件而已，帮准你用流完成文件的读写。HDFS相当于OS文件系统，HBase和Mysql一样相当于一个解析器。</p><h3 id="hbase架构图"><a class="markdownIt-Anchor" href="#hbase架构图"></a> Hbase架构图</h3><p>被网上的图片坑的很，HLog组件应该归属于HRegionServer管，但是图中却把HLog画到了HRegion中，也有可能是版本问题？</p><p><a href="https://i.loli.net/2019/06/28/5d16122ace33729145.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/06/28/5d16122ace33729145.png" alt="832D942C-0F84-4A35-B616-DE77FC9CA2CC.png"></a></p><p>下面的图摘自《HBase权威指南》</p><p><a href="https://i.loli.net/2019/06/28/5d1612c8393c545109.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/06/28/5d1612c8393c545109.png" alt="DB1A798C-2F6D-43FB-B609-FBF8824B8FFD.png"></a></p><h3 id="写操作"><a class="markdownIt-Anchor" href="#写操作"></a> 写操作</h3><p>当Client向HRegionServer发起put请求时，其将会交给对应的HRegion来处理</p><p>首先HRegion会看是否需要写入HLog（WAL用于做数据恢复和数据回滚）</p><p>当数据持久化到HLog后，数据会被直接写到MemStore中，并检查MemStore是否满了，如果满了，数据会被刷到HDFS上以HFile文件类型存储，这个操作由另一个HRegionServer的线程处理，同时会保存最后写入序号，系统就知道哪些数据被持久化了。</p><p>摘自《HBase权威指南》–8.2.2写路径</p><h3 id="读操作"><a class="markdownIt-Anchor" href="#读操作"></a> 读操作</h3><p>很纳闷为啥HBase权威指南里面没找到读数据的章节，下面摘自网络流转的说法，具体需要看一下源码验证</p><p>首先Client向HRegionServer发送Get请求，HRegion将其请求提交给对应的HRegion</p><p>HRegion会先从MemStore中找，如果找到则返回，如果没有或者数据不全，则去StoreFile中寻找</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;HDFS、MR解决了分布式存储和分布式计算问题，但是由于HDFS的随机读写能力太差，所以不能直接作为数据库。H
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="HBase" scheme="yihao.ml/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>轻办公神器</title>
    <link href="yihao.ml/2019/06/28/2019-06-28-%E8%BD%BB%E5%8A%9E%E5%85%AC%E7%A5%9E%E5%99%A8/"/>
    <id>yihao.ml/2019/06/28/2019-06-28-轻办公神器/</id>
    <published>2019-06-28T14:18:20.000Z</published>
    <updated>2019-07-02T02:01:03.771Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>推荐APP Termiuus、Working copy、Textastic</p><p><a href="https://i.loli.net/2019/06/28/5d161118d453868515.jpeg" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/06/28/5d161118d453868515.jpeg" alt="D8631229-E250-4C80-8342-EFC21F09C042.jpeg"></a></p><p><a href="https://i.loli.net/2019/06/28/5d16123cf046f65815.jpeg" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/06/28/5d16123cf046f65815.jpeg" alt="D615F0B3-A693-4B22-BDCD-373B0311B150.jpeg"></a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;推荐APP Termiuus、Working copy、Textastic&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;ht
      
    
    </summary>
    
      <category term="其他" scheme="yihao.ml/categories/%E5%85%B6%E4%BB%96/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式缓存小结</title>
    <link href="yihao.ml/2019/05/13/2019-05-13-%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98%E6%80%BB%E7%BB%93/"/>
    <id>yihao.ml/2019/05/13/2019-05-13-分布式缓存总结/</id>
    <published>2019-05-13T22:41:00.000Z</published>
    <updated>2019-07-02T02:01:03.771Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>缓存是提高服务访问速度的最有效的途径之一，下面对缓存的基本原理以及使用做一个小结。</p><h2 id="缓存的基本原理"><a class="markdownIt-Anchor" href="#缓存的基本原理"></a> 缓存的基本原理</h2><p>缓存指将数据存储在相对较高访问速度的存储介质中，以供系统处理;缓存的本质是一个内存Hash表，数据缓存是以KV形式存储在内存的Hash表中，Hash表数据读写的时间复杂度为O(1)，可以参考下图加深理解，图片摘自-《大型网站技术架构：核心原理与案例分析》</p><p><img src="https://i.loli.net/2019/05/13/5cd984ed8ac5f82181.jpeg" alt="webwxgetmsgimg (2).jpeg"></p><p>下图为是应用到代码中时的逻辑图</p><p><img src="https://i.loli.net/2019/05/13/5cd984ed8a2e348536.jpeg" alt="webwxgetmsgimg (1).jpeg"></p><h2 id="合理使用缓存"><a class="markdownIt-Anchor" href="#合理使用缓存"></a> 合理使用缓存</h2><p>缓存虽然有很多好处，但是不合理的使用缓存反而会帮倒忙，成为系统累赘。作为一个合格的开发者，有必要搞清楚其应用点，以及在应用时的注意事项，下面对其进行简单小结。</p><ul><li>频繁修改的数据</li></ul><p>频繁修改的数据不宜存入缓存，如果你这么做了，数据在存入缓存后，应用还来不及访问，就已经再次失效了，途增系统负担。</p><ul><li>没有热点访问的数据</li></ul><p>内存往往是有限的，在往内存存储时，若 redis 检测到已经没有足够空间再容纳新增加数据时，会将长期未使用的数据清理出缓存。试想一下缓存被大量非热点数据，会是怎么样的？可能数据还没有再次被访问就已经被挤出缓存。在为数据做缓存时要遵守二八原则，大部分访问的数据没有集中在小部分数据上，那么缓存就没有意义了。</p><ul><li>数据的不一致性和脏读</li></ul><p>我们会见到某购物平台店家修改了商品，但前台并未实时更新数据，这种现象称为数据的<strong>不一致和脏读</strong>。缓存内会给数据设置过­期时间，当数据过期后会重新加载数据库数­据到缓存，所以往往会有一定延时。在互联网行业中，这种延时是可以被接受的。但假如产品人员表示非要优化，那么也有应对方案，就是做实时更新同步缓存，但这种做法会带来更多的系统开销和数据一致性问题。</p><ul><li>缓存的高可用</li></ul><p>在公司中，可能会发现对于业务场景，单台 redis 缓存服务即可满足日常需要。但随着业务不断扩展，可能就会带来很多问题。比如当 redis 服务宕机时，整个服务器的业务压力会落在数据­库服务器上。这种压力的突然飙升很有可能造成服务宕机，而且这种宕机并不是简单的直接重起服务就可以解决的。对于这种问题一些人可能会使用热备服务器去解决，当主缓存服务宕掉后，自动切换到备份缓存服务器，但是这样做 违背了缓存设计的初衷，正确的做法应该是使用<strong>分布式缓存</strong>，数据会被缓存在多台机器上，当某台机器不可用时只是部分数据不可用，重新启动即可。</p><ul><li>缓存预热</li></ul><p>缓存中存放的是热点数据，热点数据又是缓存系统利用LRU算法对不断访问的数据筛选淘汰出来的，这个过程需要花费较长的时间。新启动的缓存系统如果没有任何数据，在重建缓存数据过程中，系统的性能和数据库负载都不会太好，那么最好在缓存系统启动时就把数据加载好，这种手段叫<strong>缓存的预热</strong>。例如淘宝双十一，开发人员会提前一个月预热大量的缓存数据。</p><ul><li>缓存穿透</li></ul><p>对于访问数据库没有的数据，可能一些开发人员的做法是直接返回到前台，但是这种做法是不正确的。正确的做法应是将其<strong>key缓存起来value 设置为NULL</strong>即可。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;缓存是提高服务访问速度的最有效的途径之一，下面对缓存的基本原理以及使用做一个小结。&lt;/p&gt;&lt;h2 id=&quot;缓存
      
    
    </summary>
    
      <category term="分布式" scheme="yihao.ml/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
  </entry>
  
  <entry>
    <title>Hbase ThriftServer访问内网HBase</title>
    <link href="yihao.ml/2019/05/10/2019-05-10-Hbase-ThriftServer%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91HBase/"/>
    <id>yihao.ml/2019/05/10/2019-05-10-Hbase-ThriftServer访问内网HBase/</id>
    <published>2019-05-10T17:59:00.000Z</published>
    <updated>2019-07-02T02:01:03.771Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>本地集群环境架构结构如下图所示:</p><p><img src="https://i.loli.net/2019/05/10/5cd54df270f9f.png" alt="架构图.png"></p><p>实现thriftClient与thriftServer通信，实现访问内网HBase集群</p><p>118.166.152.33和101.118.124.111 分别为公网IP,192.168.5.2/3/4分别为内网IP</p><h3 id="域名映射"><a class="markdownIt-Anchor" href="#域名映射"></a> 域名映射</h3><p>首先我们要做的是将ThriftServer服务的通信端口9000 映射到内网中，这边映射成了公网的9000端口</p><h3 id="thrift"><a class="markdownIt-Anchor" href="#thrift"></a> thrift</h3><p>下面是Thrift的百度百科</p><blockquote><p>Thrift是一种接口描述语言和二进制通讯协议，它被用来定义和创建跨语言的服务。它被当作一个远程过程调用（RPC）框架来使用，是由Facebook为“大规模跨语言服务开发”而开发的。</p></blockquote><p>Thrift支持众多通讯协议：</p><ul><li>TBinaryProtocol – 一种简单的二进制格式，简单，但没有为空间效率而优化。比文本协议处理起来更快，但更难于调试。</li><li>TCompactProtocol – 更紧凑的二进制格式，处理起来通常同样高效。</li></ul><p>想了解更多<a href="https://baike.baidu.com/item/thrift/3879058?fr=aladdin" target="_blank" rel="noopener">百度百科</a></p><p>支持的传输协议有：</p><ul><li>TFramedTransport – 当使用一个非阻塞服务器时，要求使用这个传输协议。它按帧来发送数据，其中每一帧的开头是长度信息。</li><li>TSocket – 使用阻塞的套接字I/O来传输。</li></ul><p>想了解更多<a href="https://baike.baidu.com/item/thrift/3879058?fr=aladdin" target="_blank" rel="noopener">百度百科</a></p><p>HBase ThriftServer有下面两个参数用来指定是否使用TFramedTransport协议,默认是false这边CDH中不用开启</p><p>hbase.regionserver.thrift.framed</p><blockquote><p>Use Thrift TFramedTransport on the server side. This is the recommended transport for thrift servers and requires a similar setting on the client side. Changing this to false will select the default transport, vulnerable to DoS when malformed requests are issued due to THRIFT-601.</p></blockquote><p>hbase.regionserver.thrift.compact</p><blockquote><p>Use Thrift TCompactProtocol binary serialization protocol.</p></blockquote><p>下面参数用来配置Thrift Gateway的认证，如果你配了这个东西就必须用doAs完成认证才能完成通信</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.thrift.http<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.thrift.support.proxyuser<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true/value&gt;</span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>想了解更多<a href="http://hbase.apache.org/1.2/book.html" target="_blank" rel="noopener">Configure the Thrift Gateway to Use the doAs Feature</a>，看59.6章节</p><p>我这边都没有开启如下图</p><p><img src="https://i.loli.net/2019/05/10/5cd552448d4f2.png" alt="thrift.png"></p><h3 id="client"><a class="markdownIt-Anchor" href="#client"></a> Client</h3><p>客户端可以用python或者是Java与ThriftServer进行通信。值得一提的是python3 在访问时会抛异常，这边初步查了一下也有解决方案，这边就先用python2.7进行测试，下面是代码示例:</p><p><strong>python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> common <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TSocket</span><br><span class="line"><span class="keyword">from</span> thrift.protocol <span class="keyword">import</span> TBinaryProtocol</span><br><span class="line"><span class="keyword">from</span> thrift.transport <span class="keyword">import</span> TTransport</span><br><span class="line"><span class="keyword">from</span> hbase <span class="keyword">import</span> Hbase</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connect to HBase Thrift server</span></span><br><span class="line">transport = TTransport.TBufferedTransport(TSocket.TSocket(<span class="string">"101.118.124.111"</span>, <span class="string">"9090"</span>))</span><br><span class="line">protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create and open the client connection</span></span><br><span class="line">client = Hbase.Client(protocol)</span><br><span class="line">transport.open()</span><br><span class="line"></span><br><span class="line">rows = client.getRow(<span class="string">"cars"</span>, <span class="string">"row1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> rows:</span><br><span class="line">        rowKey = row.row</span><br><span class="line">        print(<span class="string">"Got row:"</span> + rowKey);</span><br><span class="line"></span><br><span class="line"><span class="comment"># Close the client connection</span></span><br><span class="line">transport.close()</span><br></pre></td></tr></table></figure><p><strong>java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bim.hbase;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.thrift.generated.AlreadyExists;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.thrift.generated.Hbase;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.thrift.transport.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.thrift.protocol.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.thrift.protocol.TCompactProtocol;</span><br><span class="line"><span class="keyword">import</span> org.apache.thrift.transport.TFramedTransport;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.Charset;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HbaseThriftTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String host;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Integer port;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        InputStream in = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            in = HbaseThriftTest.class.getClassLoader().getResourceAsStream(<span class="string">"system.properties"</span>);</span><br><span class="line">            properties.load(in);</span><br><span class="line">            host = properties.getProperty(<span class="string">"hbase.thrift.host"</span>);</span><br><span class="line">            port = Integer.parseInt(properties.getProperty(<span class="string">"hbase.thrift.port"</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">if</span>(in != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    in.close();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        init();</span><br><span class="line">        String Proto = <span class="string">"binary"</span>;</span><br><span class="line">        String TableName = <span class="string">"t1"</span>;</span><br><span class="line">        String ColFamily = <span class="string">"rowkey002"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// setup the hbase thrift connection</span></span><br><span class="line">        TTransport Transport;</span><br><span class="line">        Transport = <span class="keyword">new</span> TSocket(host, port);</span><br><span class="line">        TCompactProtocol FProtocol = <span class="keyword">new</span> TCompactProtocol(Transport);</span><br><span class="line">        Hbase.Client Client = <span class="keyword">new</span> Hbase.Client(FProtocol);</span><br><span class="line">        <span class="keyword">if</span> (Proto.equals(<span class="string">"binary"</span>)) &#123;</span><br><span class="line">            TProtocol Protocol = <span class="keyword">new</span> TBinaryProtocol(Transport, <span class="keyword">true</span>, <span class="keyword">true</span>);</span><br><span class="line">            Client = <span class="keyword">new</span> Hbase.Client(Protocol);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ( Proto.equals(<span class="string">"framed"</span>)) &#123;</span><br><span class="line">            Transport = <span class="keyword">new</span> TFramedTransport(<span class="keyword">new</span> TSocket(host, port));</span><br><span class="line">            TProtocol Protocol = <span class="keyword">new</span> TBinaryProtocol(Transport, <span class="keyword">true</span>, <span class="keyword">true</span>);</span><br><span class="line">            Client = <span class="keyword">new</span> Hbase.Client(Protocol);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ( ! Proto.equals(<span class="string">"compact"</span>)) &#123;</span><br><span class="line">            System.out.println(<span class="string">"Protocol must be compact or framed or binary"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        Transport.open();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// prepare the column family</span></span><br><span class="line">        List&lt;ColumnDescriptor&gt; Columns = <span class="keyword">new</span> ArrayList&lt;ColumnDescriptor&gt;();</span><br><span class="line">        ColumnDescriptor col = <span class="keyword">new</span> ColumnDescriptor();</span><br><span class="line">        col.name = ByteBuffer.wrap(ColFamily.getBytes());</span><br><span class="line">        Columns.add(col);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// dump existing tables</span></span><br><span class="line">        System.out.println(<span class="string">"#~ Dumping Existing tables"</span>);</span><br><span class="line">        <span class="keyword">for</span> (ByteBuffer tn : Client.getTableNames()) &#123;</span><br><span class="line">            System.out.println(<span class="string">"-- found: "</span> + <span class="keyword">new</span> String(tn.array(), Charset.forName(<span class="string">"UTF-8"</span>)));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// create the new table</span></span><br><span class="line">        System.out.println(<span class="string">"#~ Creating table: "</span> + TableName);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Client.createTable(ByteBuffer.wrap(TableName.getBytes()), Columns);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (AlreadyExists ae) &#123;</span><br><span class="line">            System.out.println(<span class="string">"WARN: "</span> + ae.message);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Transport.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是system.properties</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase.thrift.host=101.118.124.111</span><br><span class="line">hbase.thrift.port=9090</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;本地集群环境架构结构如下图所示:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/201
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hbase" scheme="yihao.ml/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统的知识结构总结</title>
    <link href="yihao.ml/2019/05/09/2019-05-09-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%9F%A5%E8%AF%86%E7%BB%93%E6%9E%84%E6%80%BB%E7%BB%93/"/>
    <id>yihao.ml/2019/05/09/2019-05-09-分布式系统的知识结构总结/</id>
    <published>2019-05-09T12:47:14.000Z</published>
    <updated>2019-07-02T02:01:03.771Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>构件分布式系统的目的是增加系统容量，提高系统可用性。转换成技术方面就是完成下面两件事</p><ul><li><p>大流量处理。所谓大流量处理就是利用集群技术将大量的并发请求分发到不同机器上</p></li><li><p>关键业务保护。提高系统可用性，所以需要将故障隔离起来，防止雪崩效应引起的整体服务无法正常服务。</p></li></ul><p>说白了就是干两件事，一是提高系统架构的吞吐量，服务更多的并发流量；二是为了提高系统的稳定性，让系统的可用性更高；下面从<strong>系统的性能</strong>和<strong>系统的稳定性</strong>来说一下在分布式下需要完成的事情。</p><h3 id="系统性能"><a class="markdownIt-Anchor" href="#系统性能"></a> 系统性能</h3><p>系统性能可以从下面五个方面入手来做整体优化，他们分别是：</p><ul><li>缓存</li><li>负载均衡</li><li>异步</li><li>数据分区</li><li>数据镜像</li></ul><h4 id="缓存"><a class="markdownIt-Anchor" href="#缓存"></a> 缓存</h4><p>从前台到后台再到数据库，都有缓存。缓存是提高服务响应速度的最直接手段，在分布式环境中，可以使用MemCache、redis来构件分布式缓存。目前从市面上看来大家都更喜欢redis，这其中需要一个Proxy来做缓存的分片和路由。</p><h4 id="负载均衡"><a class="markdownIt-Anchor" href="#负载均衡"></a> 负载均衡</h4><p>负载均衡是水平拓展的关键技术，它可以是多台机器共同分担一部分流量请求。</p><h4 id="异步"><a class="markdownIt-Anchor" href="#异步"></a> 异步</h4><p>异步这块主要是通过异步队列对请求做排队处理，这边有很多业务场景，比如可以把前端并发请求的峰值给“削平”了，让后端通过自己能够处理的速度来处理请求，进而来增加系统的吞吐量，但这通常比较适用于实时性不是很高的场景；引入消息队列后，可能会出现消息丢失的问题，这就被迫我们不得不去做消息的持久化，持久化会造成有“状态”的节点，从而增加服务调度的难度。</p><h4 id="数据分区镜像"><a class="markdownIt-Anchor" href="#数据分区镜像"></a> 数据分区/镜像</h4><p>数据分区和数据镜像可以放在一起，数据分区就是将数据按照某种固定的方式分成多个区。比如按照地区来分，这样需要一个数据路由的中间件，不同地区来访问不同区域的数据库，来减少数据库的压力。但是这样会造成跨库的join和跨库的事务异常复杂。而数据镜像是将数据复制成多分，这样就不需要数据中间件了，可以在任意节点上进行读写， 内部会进行自动数据同步，但是数据镜像中最大的问题就是数据一致性问题。</p><p>对于一般公司来说，在初期会使用读写分离的数据镜像方式，然而后期会采用分库分表的方式。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;构件分布式系统的目的是增加系统容量，提高系统可用性。转换成技术方面就是完成下面两件事&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;
      
    
    </summary>
    
      <category term="分布式" scheme="yihao.ml/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
  </entry>
  
  <entry>
    <title>CDH5.15 权限管理</title>
    <link href="yihao.ml/2019/04/30/2019-04-30-CDH-%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"/>
    <id>yihao.ml/2019/04/30/2019-04-30-CDH-权限管理/</id>
    <published>2019-04-30T13:37:30.000Z</published>
    <updated>2019-07-02T02:01:03.771Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>CDH默认会给我们创建很多角色用户，这边对于开发和管理来说十分不方便。这边将所有的用户修改为ROOT</p><h3 id="hdfs"><a class="markdownIt-Anchor" href="#hdfs"></a> HDFS</h3><p>下面描述一下怎么修改HDFS</p><p><img src="https://i.loli.net/2019/04/30/5cc7f41d6fedb.png" alt="1.png"><br><img src="https://i.loli.net/2019/04/30/5cc7f41d76977.png" alt="2.png"><br><img src="https://i.loli.net/2019/04/30/5cc7f41d5ef76.png" alt="3.png"></p><p>执行下面指令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown -R root:root /var/run/hdfs-sockets</span><br></pre></td></tr></table></figure><h3 id="yarn"><a class="markdownIt-Anchor" href="#yarn"></a> YARN</h3><p>搜索“用户”、“系统组”、“mapred”、“hadoop”修改为root</p><p><img src="https://i.loli.net/2019/04/30/5cc7f4ff9cc06.png" alt="48.png"></p><h3 id="其他组件"><a class="markdownIt-Anchor" href="#其他组件"></a> 其他组件</h3><p>最后修改ZooKeeper，Hive，Impala，Hbase，Spark，Sqoop2，Flume，Kafka等的用户和系统组为root。</p><p>搜索“用户”、“组”修改为root</p><h3 id="将角色加到root组中"><a class="markdownIt-Anchor" href="#将角色加到root组中"></a> 将角色加到root组中</h3><p>CDH给我们建的角色在/etc/passwd中可以体现</p><p>找出所有的相关角色把他们加到root组中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">usermod -a -G datacenter hue</span><br><span class="line">usermod -a -G datacenter yarn</span><br><span class="line">usermod -a -G datacenter flume</span><br><span class="line">usermod -a -G datacenter impala</span><br><span class="line">usermod -a -G datacenter spark</span><br><span class="line">usermod -a -G datacenter zookeeper</span><br><span class="line">usermod -a -G datacenter mapred</span><br><span class="line">usermod -a -G datacenter sqoop</span><br><span class="line">usermod -a -G datacenter hive</span><br><span class="line">usermod -a -G datacenter sqoop2</span><br><span class="line">usermod -a -G datacenter oozie</span><br><span class="line">usermod -a -G datacenter hbase</span><br><span class="line">usermod -a -G datacenter hdfs</span><br><span class="line">usermod -a -G datacenter kudu</span><br><span class="line">usermod -a -G datacenter httpfs</span><br><span class="line">usermod -a -G datacenter root</span><br></pre></td></tr></table></figure><h3 id="修改hdfs"><a class="markdownIt-Anchor" href="#修改hdfs"></a> 修改HDFS</h3><p>hdfs dfs -chown -R root:root /</p><p>重启集群服务即可，后续如果还有权限问题需要按提示进行修改即可</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;CDH默认会给我们创建很多角色用户，这边对于开发和管理来说十分不方便。这边将所有的用户修改为ROOT&lt;/p&gt;&lt;
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="CDH" scheme="yihao.ml/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>chrome auto proxy</title>
    <link href="yihao.ml/2019/04/28/2019-04-28-chrome-auto-proxy/"/>
    <id>yihao.ml/2019/04/28/2019-04-28-chrome-auto-proxy/</id>
    <published>2019-04-28T15:39:39.000Z</published>
    <updated>2019-07-02T02:01:03.770Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>到google应用商店下载Proxy SwitchyOmega 下面进行相关配置</p><p><a href="https://i.loli.net/2019/04/28/5cc55a54b0cc0.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/04/28/5cc55a54b0cc0.png" alt="1.png"></a></p><p><a href="https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt" target="_blank" rel="noopener">点我下载</a></p><p><a href="https://i.loli.net/2019/04/28/5cc55a54cca52.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/04/28/5cc55a54cca52.png" alt="2.png"></a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;到google应用商店下载Proxy SwitchyOmega 下面进行相关配置&lt;/p&gt;&lt;p&gt;&lt;a href=
      
    
    </summary>
    
      <category term="其他" scheme="yihao.ml/categories/%E5%85%B6%E4%BB%96/"/>
    
    
  </entry>
  
  <entry>
    <title>CDH5.15.0 hbase hue 配置后Error</title>
    <link href="yihao.ml/2019/04/27/2019-04-27-CDH15-5-0-hbase-hue/"/>
    <id>yihao.ml/2019/04/27/2019-04-27-CDH15-5-0-hbase-hue/</id>
    <published>2019-04-27T17:43:28.000Z</published>
    <updated>2019-07-02T02:01:03.770Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>在CDM hue的配置界面搜索 hue_safety将下面代码加入到“值”</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hbase]</span><br><span class="line">hbase_conf_dir=&#123;&#123;HBASE_CONF_DIR&#125;&#125;</span><br><span class="line">thrift_transport=buffered</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/30/5d18569ef1e8014353.jpg" alt></p><p>在CDM Hbase的配置界面搜索 core-site.xml将下面代码加入到“值”</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hbase.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hbase.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>除此之外想看更多配置可以参考 <a href="https://blog.csdn.net/zhangshenghang/article/details/85776134" target="_blank" rel="noopener">https://blog.csdn.net/zhangshenghang/article/details/85776134</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;在CDM hue的配置界面搜索 hue_safety将下面代码加入到“值”&lt;/p&gt;&lt;figure class=
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hue" scheme="yihao.ml/tags/Hue/"/>
    
  </entry>
  
  <entry>
    <title>TProtocolException: Bad version in readMessageBegin</title>
    <link href="yihao.ml/2019/04/26/2019-04-26-TProtocolException-Bad-version-in-readMessageBegin/"/>
    <id>yihao.ml/2019/04/26/2019-04-26-TProtocolException-Bad-version-in-readMessageBegin/</id>
    <published>2019-04-26T13:23:25.000Z</published>
    <updated>2019-07-02T02:01:03.770Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>链接thrift异常</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">org.apache.thrift.protocol.TProtocolException: Bad version in readMessageBegin</span><br><span class="line">        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:<span class="number">223</span>)</span><br><span class="line">        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:<span class="number">27</span>)</span><br><span class="line">        at org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:<span class="number">289</span>)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:<span class="number">1149</span>)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:<span class="number">624</span>)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:<span class="number">748</span>)</span><br><span class="line"><span class="number">2019</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">04</span>:<span class="number">02</span>,<span class="number">472</span> ERROR org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer: Thrift error occurred during processing of message.</span><br><span class="line">org.apache.thrift.protocol.TProtocolException: Bad version in readMessageBegin</span><br><span class="line">        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:<span class="number">223</span>)</span><br><span class="line">        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:<span class="number">27</span>)</span><br><span class="line">        at org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:<span class="number">289</span>)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:<span class="number">1149</span>)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:<span class="number">624</span>)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:<span class="number">748</span>)</span><br></pre></td></tr></table></figure><p>去CDH中的Hbase面板中检查<em>hbase.regionserver.thrift.compact</em>和<em>hbase.regionserver.thrift.framed</em>是否启用</p><p>hbase.regionserver.thrift.framed</p><p>Description:</p><blockquote><p>Use Thrift TFramedTransport on the server side. This is the recommended transport for thrift servers and requires a similar setting on the client side. Changing this to false will select the default transport, vulnerable to DoS when malformed requests are issued due to THRIFT-601.</p></blockquote><p>hbase.regionserver.thrift.compact</p><p>Description</p><blockquote><p>Use Thrift TCompactProtocol binary serialization protocol.</p></blockquote><p>增加ThriftServer堆栈大小到2G并保存</p><p>重启ThriftServer服务即可</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;链接thrift异常&lt;/p&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hbase" scheme="yihao.ml/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>CDH Error: JAVA_HOME is not set and could not be found.</title>
    <link href="yihao.ml/2019/04/26/2019-04-26-CDH-Error-JAVA-HOME-is-not-set-and-could-not-be-found/"/>
    <id>yihao.ml/2019/04/26/2019-04-26-CDH-Error-JAVA-HOME-is-not-set-and-could-not-be-found/</id>
    <published>2019-04-26T13:23:25.000Z</published>
    <updated>2019-07-02T02:01:03.770Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><h3 id="错误一"><a class="markdownIt-Anchor" href="#错误一"></a> 错误一</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /flume/mysql</span><br><span class="line">Permission denied: user=root, access=WRITE, inode="/":hdfs:supergroup:drwxr-xr-x</span><br></pre></td></tr></table></figure><p>执行命令的用户没有执行权限。直接给当前用户授权。（这种想法是不正确的，不要为了简化输入命令，就试图修改这些东西）正确的做法应该是。切换指定用户执行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cdh1 data]#sudo -u hdfs  hadoop fs -mkdir /newFile</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cdh1 data]#sudo -u hdfs  dfhs dfs  -mkdir /newFile</span><br></pre></td></tr></table></figure><p>更简单的是，先进入这个用户，su hdfs</p><h3 id="错误二"><a class="markdownIt-Anchor" href="#错误二"></a> 错误二</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo -u hdfs hdfs dfs -mkdir -p /flume/mysql  </span><br><span class="line">Error: JAVA_HOME is not set and could not be found.</span><br><span class="line">java -version</span><br><span class="line">java version "1.8.0_91"</span><br></pre></td></tr></table></figure><p>确实已经设置了JAVA_HOME ，而且在linux shell 执行 echo $JAVA_HOME  也是有输出。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">find / -name cloudera-config.sh</span><br><span class="line">/*/*/*/cloudera-manager/cm-5.10.0/lib64/cmf/service/common/cloudera-config.sh</span><br><span class="line">local JAVA8_HOME_CANDIDATES=(</span><br><span class="line">    '/usr/java/jdk1.8'</span><br><span class="line">    '/usr/java/jre1.8'</span><br><span class="line">    '/usr/lib/jvm/j2sdk1.8-oracle'</span><br><span class="line">    '/usr/lib/jvm/j2sdk1.8-oracle/jre'</span><br><span class="line">    '/usr/lib/jvm/java-8-oracle'</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>解决办法:</p><p>建立一个已经有的JAVA_HOME  链接到 /usr/java/jdk1.8 就好了！<br>目标位置：/usr/java/jdk1.8<br>原文件：/<em>/</em>/jdk1.8.0_91</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ln -s 源文件 目标文件</span><br><span class="line">ln -s /*/*/jdk1.8.0_91 /usr/java/jdk1.8</span><br><span class="line">sudo -u hdfs hdfs dfs -mkdir -p /flume/mysql</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;h3 id=&quot;错误一&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#错误一&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="CDH" scheme="yihao.ml/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>RDD DataSet和DataFrame的区别和应用场景</title>
    <link href="yihao.ml/2019/04/24/2019-04-24-RDD-DataSet%E5%92%8CDataFrame%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/"/>
    <id>yihao.ml/2019/04/24/2019-04-24-RDD-DataSet和DataFrame的区别和应用场景/</id>
    <published>2019-04-24T11:47:37.000Z</published>
    <updated>2019-07-02T02:01:03.769Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>在spark中，RDD、DataFrame、Dataset是最常用的数据类型，本博文给出笔者在使用的过程中体会到的区别和各自的优势。</p><h2 id="共性"><a class="markdownIt-Anchor" href="#共性"></a> 共性</h2><p>1、 RDD、DataFrame和Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利；</p><p>2、 三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过，如</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkconf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"test"</span>).set(<span class="string">"spark.port.maxRetries"</span>,<span class="string">"1000"</span>)</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkconf).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> rdd=spark.sparkContext.parallelize(<span class="type">Seq</span>((<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>)))</span><br><span class="line">rdd.map&#123;line=&gt;</span><br><span class="line">    println(<span class="string">"运行"</span>)</span><br><span class="line">    line._1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>map中的println(“运行”)并不会运行。</p><p>3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出。</p><p>4、三者都有partition的概念</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> predata=data.repartition(<span class="number">24</span>).mapPartitions&#123;</span><br><span class="line">    <span class="type">PartLine</span> =&gt; &#123;</span><br><span class="line">        <span class="type">PartLine</span>.map&#123;</span><br><span class="line">            line =&gt;</span><br><span class="line">            println(“转换操作”)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样对每一个分区进行操作时，就跟在操作数组一样，不但数据量比较小，而且可以方便的将map中的运算结果拿出来，如果直接用map，map中对外面的操作是无效的</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd=spark.sparkContext.parallelize(<span class="type">Seq</span>((<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="keyword">var</span> flag=<span class="number">0</span></span><br><span class="line"><span class="keyword">val</span> test=rdd.map&#123;line=&gt;</span><br><span class="line">    println(<span class="string">"运行"</span>)</span><br><span class="line">    flag+=<span class="number">1</span></span><br><span class="line">    println(flag)</span><br><span class="line">    line._1</span><br><span class="line">&#125;</span><br><span class="line">println(test.count)</span><br><span class="line">println(flag)</span><br></pre></td></tr></table></figure><p>结果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">运行</span><br><span class="line">1</span><br><span class="line">运行</span><br><span class="line">2</span><br><span class="line">运行</span><br><span class="line">3</span><br><span class="line">3</span><br><span class="line">0</span><br></pre></td></tr></table></figure><p>不使用partition时，对map之外的操作无法对map之外的变量造成影响。</p><p>5、三者有许多共同的函数，如filter，排序等。</p><p>6、在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//这里的spark是SparkSession的变量名</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>7、DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型。</p><p>DataFrame:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">testDF.map&#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Row</span>(col1:<span class="type">String</span>,col2:<span class="type">Int</span>)=&gt;</span><br><span class="line">    println(col1);println(col2)</span><br><span class="line">    col1</span><br><span class="line">    <span class="keyword">case</span> _=&gt;</span><br><span class="line">    <span class="string">""</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>为了提高稳健性，最好后面有一个_通配操作，这里提供了DataFrame一个解析字段的方法。</p><p>Dataset:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">testDS</span>.<span class="title">map</span></span>&#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Coltest</span>(col1:<span class="type">String</span>,col2:<span class="type">Int</span>)=&gt;</span><br><span class="line">    println(col1);println(col2)</span><br><span class="line">    col1</span><br><span class="line">    <span class="keyword">case</span> _=&gt;</span><br><span class="line">    <span class="string">""</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="区别"><a class="markdownIt-Anchor" href="#区别"></a> 区别</h2><h4 id="rdd"><a class="markdownIt-Anchor" href="#rdd"></a> RDD</h4><p>1、RDD一般和spark mlib同时使用。</p><p>2、RDD不支持sparkSQL操作。</p><p>DataFrame:</p><p>1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，如</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">testDF.foreach&#123;</span><br><span class="line">    line =&gt;</span><br><span class="line">    <span class="keyword">val</span> col1=line.getAs[<span class="type">String</span>](<span class="string">"col1"</span>)</span><br><span class="line">    <span class="keyword">val</span> col2=line.getAs[<span class="type">String</span>](<span class="string">"col2"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每一列的值没法直接访问。</p><p>2、DataFrame与Dataset一般与spark ml同时使用。</p><p>3、DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作，如</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataDF.createOrReplaceTempView(<span class="string">"tmp"</span>)</span><br><span class="line">spark.sql(<span class="string">"select ROW,DATE from tmp where DATE is not null order by DATE"</span>).show(<span class="number">100</span>,<span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>4、DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//保存</span></span><br><span class="line"><span class="keyword">val</span> saveoptions = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://172.xx.xx.xx:9000/test"</span>)</span><br><span class="line">datawDF.write.format(<span class="string">"com.databricks.spark.csv"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).options(saveoptions).save()</span><br><span class="line"><span class="comment">//读取</span></span><br><span class="line"><span class="keyword">val</span> options = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://172.xx.xx.xx:9000/test"</span>)</span><br><span class="line"><span class="keyword">val</span> datarDF= spark.read.options(options).format(<span class="string">"com.databricks.spark.csv"</span>).load()</span><br></pre></td></tr></table></figure><p>利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定。</p><p>Dataset:</p><p>这里主要对比Dataset和DataFrame，因为Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。</p><p>DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。</p><p>而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">//rdd</span></span></span><br><span class="line"><span class="class"><span class="title">//</span>(<span class="params">"a", 1</span>)</span></span><br><span class="line"><span class="class"><span class="title">//</span>(<span class="params">"b", 1</span>)</span></span><br><span class="line"><span class="class"><span class="title">//</span>(<span class="params">"a", 1</span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">test</span></span>: <span class="type">Dataset</span>[<span class="type">Coltest</span>]=rdd.map&#123;line=&gt;</span><br><span class="line">    <span class="type">Coltest</span>(line._1,line._2)</span><br><span class="line">&#125;.toDS</span><br><span class="line">test.map&#123;</span><br><span class="line">    line=&gt;</span><br><span class="line">    println(line.col1)</span><br><span class="line">    println(line.col2)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。</p><p>转化：</p><p>RDD、DataFrame、Dataset三者有许多共性，有各自适用的场景常常需要在三者之间转换。</p><p>DataFrame/Dataset转RDD：</p><p>这个转换很简单</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1=testDF.rdd</span><br><span class="line"><span class="keyword">val</span> rdd2=testDS.rdd</span><br></pre></td></tr></table></figure><p>RDD转DataFrame：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> testDF = rdd.map &#123;line=&gt;</span><br><span class="line">    (line._1,line._2)</span><br><span class="line">&#125;.toDF(<span class="string">"col1"</span>,<span class="string">"col2"</span>)</span><br></pre></td></tr></table></figure><p>一般用元组把一行的数据写在一起，然后在toDF中指定字段名。</p><p>RDD转Dataset：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">testDS</span> </span>= rdd.map &#123;line=&gt;</span><br><span class="line">    <span class="type">Coltest</span>(line._1,line._2)</span><br><span class="line">&#125;.toDS</span><br></pre></td></tr></table></figure><p>可以注意到，定义每一行的类型（case class）时，已经给出了字段名和类型，后面只要往case class里面添加值即可。</p><p>Dataset转DataFrame：</p><p>这个也很简单，因为只是把case class封装成Row。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> testDF = testDS.toDF</span><br></pre></td></tr></table></figure><p>DataFrame转Dataset：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">testDS</span> </span>= testDF.as[<span class="type">Coltest</span>]</span><br></pre></td></tr></table></figure><p>这种方法就是在给出每一列的类型后，使用as方法，转成Dataset，这在数据类型是DataFrame又需要针对各个字段处理时极为方便。</p><p>特别注意：</p><p>在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;在spark中，RDD、DataFrame、Dataset是最常用的数据类型，本博文给出笔者在使用的过程中体会
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="yihao.ml/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>CDH5.15.0升级spark1.6到2.3</title>
    <link href="yihao.ml/2019/04/23/2019-04-23-CDH5-15%E5%8D%87%E7%BA%A7spark1-6%E5%88%B02-3/"/>
    <id>yihao.ml/2019/04/23/2019-04-23-CDH5-15升级spark1-6到2-3/</id>
    <published>2019-04-23T20:10:32.000Z</published>
    <updated>2019-07-02T02:01:03.769Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>CDH5.15.0安装集群以后，默认安装的spark是1.6版本。添加的时候没有spark2,因为spark1.6好多新功能都不能使用，所以这边对其进行升级。</p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIgy1g2ctw7eeuzj313u0bd0u5.jpg" alt></p><h2 id="安装包"><a class="markdownIt-Anchor" href="#安装包"></a> 安装包</h2><ul><li>parcel、parcel.sha和manifest.json</li><li>csd</li></ul><p>下载parcel等文件<a href="http://archive.cloudera.com/spark2/parcels/latest/" target="_blank" rel="noopener">点我下载</a></p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIgy1g2cu3l8hrxj30mt0j5mz6.jpg" alt></p><p>下载csd文件<a href="http://archive.cloudera.com/spark2/csd/" target="_blank" rel="noopener">点我下载</a></p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIgy1g2cu77pn4cj30gc0fngmk.jpg" alt></p><blockquote><p>关于版本，csd和parcel的版本要对应上本例子中都是cloudera1;parcel的版本要选择适合自己操作系统的，本例中使用的是centos7,所以下载el7</p></blockquote><p>下载好所有文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SPARK2_ON_YARN-2.4.0.cloudera1.jar</span><br><span class="line">SPARK2-2.4.0.cloudera1-1.cdh5.13.3.p0.1007356-el7.parcel</span><br><span class="line">SPARK2-2.4.0.cloudera1-1.cdh5.13.3.p0.1007356-el7.parcel.sha1</span><br><span class="line">manifest.json</span><br></pre></td></tr></table></figure><h2 id="上传"><a class="markdownIt-Anchor" href="#上传"></a> 上传</h2><p>将SPARK2_ON_YARN-2.4.0.cloudera1.jar上传到主节点的*/opt/cloudera/csd/*</p><blockquote><p>没有目录的话创建一个</p></blockquote><p>将其余文件上传到主节点的*/opt/cloudera/parcel-repo/*</p><blockquote><p>目录下面如果有重名文件必须删掉，如果没有则不用管</p></blockquote><h2 id="重启csm"><a class="markdownIt-Anchor" href="#重启csm"></a> 重启CSM</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在主节点运行</span></span><br><span class="line">/opt/cm-5.15.0/etc/init.d/cloudera-scm-server start</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在所有节点运行</span></span><br><span class="line">/opt/cm-5.15.0/etc/init.d/cloudera-scm-agent start</span><br></pre></td></tr></table></figure><h2 id="激活安装"><a class="markdownIt-Anchor" href="#激活安装"></a> 激活安装</h2><p>到cloudera manager界面 主机-》parcel-》SPARK2 做激活</p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIgy1g2cugy2z59j31gb0pb0v8.jpg" alt></p><p>按照正常操作添加SPARK2到集群即可</p><h2 id="验证"><a class="markdownIt-Anchor" href="#验证"></a> 验证</h2><p>到node1节点下运行<em>spark-shell</em>发现报错</p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIgy1g2cum7ztb3j30x90cpjsg.jpg" alt></p><p>首先我们得知道下面这些事情</p><ul><li>CDH安装目录 /opt/cloudera/parcels/CDH/</li><li>SPARK2安装目录 /opt/cloudera/parcels/SPARK2</li><li>所有配置文件目录为 /etc/</li></ul><p>将CDH中spark配置文件拷贝到SPARK2的配置文件中,并配置spark-env.sh文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /opt/cloudera/parcels/CDH/etc/spark/conf.dist/* /opt/cloudera/parcels/SPARK2/etc/spark2/conf.dist/</span><br></pre></td></tr></table></figure><p>检查一下配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /opt/cloudera/parcels/SPARK2/etc/spark2/conf.dist/spark-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加下面内容到*spark-env.sh*中</span></span><br><span class="line">export SPARK_DIST_CLASSPATH=$(hadoop classpath) //指定hadoop class文件目录</span><br><span class="line">export HADOOP_CONF_DIR=/etc/hadoop/conf //指定hadoop配置文件目录</span><br></pre></td></tr></table></figure><p>将Spark2加入到环境变量中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">添加如下内容</span></span><br><span class="line">export HADOOP_CONF_DIR=/etc/hadoop/conf</span><br><span class="line">export SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2</span><br><span class="line">export PATH=$SPARK_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><h4 id="spark-on-yarn测试"><a class="markdownIt-Anchor" href="#spark-on-yarn测试"></a> spark on yarn测试</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/cloudera/parcels/SPARK2/lib/spark2/examples/jars</span><br><span class="line"></span><br><span class="line">spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --queue thequeue \</span><br><span class="line">    ./spark-examples_2.11-2.4.0.cloudera1.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure><p>到yarn上查看任务http://zhaoyihao.iok.la:8088/cluster</p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIgy1g2cuxpg2foj31gn0p7wh8.jpg" alt></p><h4 id="spark-sql-操作hive测试"><a class="markdownIt-Anchor" href="#spark-sql-操作hive测试"></a> Spark SQL 操作Hive测试</h4><p>这里有一个参数特别重要<em>spark.sql.warehouse.dir</em>下面是官方解释</p><blockquote><p>When working with Hive, one must instantiate SparkSession with Hive support, including connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions. Users who do not have an existing Hive deployment can still enable Hive support. When not configured by the hive-site.xml, the context automatically creates metastore_db in the current directory and creates a directory configured by spark.sql.warehouse.dir, which defaults to the directory spark-warehouse in the current directory that the Spark application is started. Note that the hive.metastore.warehouse.dir property in hive-site.xml is deprecated since Spark 2.0.0. Instead, use spark.sql.warehouse.dir to specify the default location of database in warehouse. You may need to grant write privilege to the user who starts the Spark application.</p></blockquote><p>大概的意思是，使用hive需要sparksession设置支持选项，如果用户集群里，没有部署好的hive，sparksession也能够提供hive支持，在这种情况下，如果没有hive-site.xml文件，sparkcontext会自动在当前目录创建元数据db,并且会在spark.sql.warehouse.dir表示的位置创建一个目录，用户存放table数据，所以spark.sql.warehouse.dir是一个用户存放hive table文件的一个目录，因为是一个目录地址，难免会收到操作系统的影响，因为不同的文件系统的前缀是不一样了，为了适配性，spark鼓励在code中设置该选项，而不是在hive-site.xml中设置该选项。</p><p>1.如果没有部署好的hive，spark确实是会使用内置的hive，但是spark会将所有的元信息都放到spark_home/bin 目录下，也就是为什么配置了spark.sql.warehouse.dir 却不起作用的原因。而且，就算部署了hive，也需要让spark识别hive，否则spark，还是会使用spark默认的hive</p><p>2.只有在部署好的hive情况下，使用spark.sql.warehouse.dir才会生效，而且spark会默认覆盖hive的配置项。</p><p>下面摘自官方文档</p><blockquote><p>Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and hdfs-site.xml (for HDFS configuration) file in conf/. <a href="http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html</a></p></blockquote><p>到cloudera manager 下载hvie的客户端配置，将hive-site.xml，core-site.xml，hdfs-site.xml复制到*/opt/cloudera/parcels/SPARK2-2.4.0.cloudera1-1.cdh5.13.3.p0.1007356/lib/spark2/conf*目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/hadoop/conf/hdfs-site.xml /opt/cloudera/parcels/SPARK2/lib/spark2/conf/</span><br><span class="line">cp /etc/hadoop/conf/core-site.xml /opt/cloudera/parcels/SPARK2/lib/spark2/conf/</span><br><span class="line">cp /etc/hive/conf/hive-site.xml /opt/cloudera/parcels/SPARK2/lib/spark2/conf/</span><br></pre></td></tr></table></figure><p>进入spark-shell</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .appName(<span class="string">"Spark Hive Example"</span>)</span><br><span class="line">    <span class="comment">//在实例化sparkSession时指定hive的warehouse</span></span><br><span class="line">    .config(<span class="string">"spark.sql.warehouse.dir"</span>, <span class="string">"/user/hive/warehouse"</span>)</span><br><span class="line">    <span class="comment">//调用enableHiveSupport开启hive的支持</span></span><br><span class="line">    .enableHiveSupport()</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.sql</span><br><span class="line"></span><br><span class="line">sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"</span>)</span><br><span class="line">sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries are expressed in HiveQL</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM src"</span>).show()</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;CDH5.15.0安装集群以后，默认安装的spark是1.6版本。添加的时候没有spark2,因为spark1
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="CDH" scheme="yihao.ml/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH5.15.x 启动服务Time out</title>
    <link href="yihao.ml/2019/04/17/2019-04-17-CDH5-15-x-time-out/"/>
    <id>yihao.ml/2019/04/17/2019-04-17-CDH5-15-x-time-out/</id>
    <published>2019-04-17T19:02:00.000Z</published>
    <updated>2019-07-02T02:01:03.769Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>环境: CDH 5.15.0 + centos7</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Command aborted because of exception: Command timed-out after 150 seconds</span><br></pre></td></tr></table></figure><p>这是由于服务端集群未禁用ipv6导致</p><p>使用ifconfig命令查看网卡信息，如果出现<strong>inet6 fe80::20c:29ff:fed0:3514</strong>，说明机器开启了ipv6<br><img src="https://i.loli.net/2019/04/27/5cc427c43c480.png" alt="1.png"></p><p>编辑**/etc/sysctl.conf**配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.ipv6.conf.all.disable_ipv6=1</span><br></pre></td></tr></table></figure><p>编辑**/etc/sysconfig/network**配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NETWORKING_IPV6=no</span><br></pre></td></tr></table></figure><p>编辑**/etc/sysconfig/network-scripts/ifcfg-eno16777736**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IPV6INIT=no</span><br></pre></td></tr></table></figure><p>执行sysctl -p或者reboot重启命令</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;环境: CDH 5.15.0 + centos7&lt;/p&gt;&lt;figure class=&quot;highlight pl
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="CDH" scheme="yihao.ml/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>java.lang.NoClassDefFoundError scala/Product$class</title>
    <link href="yihao.ml/2019/04/16/2019-04-16-spark-idea-windows-scala-Product/"/>
    <id>yihao.ml/2019/04/16/2019-04-16-spark-idea-windows-scala-Product/</id>
    <published>2019-04-16T13:18:58.000Z</published>
    <updated>2019-07-02T02:01:03.768Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>环境：windows 7 + idea + scala + spark</p><p>本地运行以后报下面错误</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> java.lang.NoClassDefFoundError: scala/Product$<span class="class"><span class="keyword">class</span></span></span><br><span class="line">at org.apache.spark.SparkConf$DeprecatedConfig.&lt;init&gt;(SparkConf.scala:682)</span><br><span class="line">at org.apache.spark.SparkConf$.&lt;init&gt;(SparkConf.scala:<span class="number">539</span>)</span><br><span class="line">at org.apache.spark.SparkConf$.&lt;clinit&gt;(SparkConf.scala)</span><br><span class="line">at org.apache.spark.SparkConf.set(SparkConf.scala:<span class="number">72</span>)</span><br><span class="line">at org.apache.spark.SparkConf.setAppName(SparkConf.scala:<span class="number">87</span>)</span><br><span class="line">at com.bim.WordCount$.main(WordCount.scala:<span class="number">9</span>)</span><br><span class="line">at com.bim.WordCount.main(WordCount.scala)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: scala.Product$<span class="class"><span class="keyword">class</span></span></span><br><span class="line">at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:<span class="number">424</span>)</span><br><span class="line">at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:<span class="number">335</span>)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:<span class="number">357</span>)</span><br><span class="line">... <span class="number">7</span> more</span><br></pre></td></tr></table></figure><p>Spark和Scala的版本是有对应关系的，下面有个查看关系的小技巧，去<a href="https://mvnrepository.com/" target="_blank" rel="noopener">https://mvnrepository.com/</a>中搜索<strong>spark</strong>，进入<strong>Spark Project Core</strong>查看即可</p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIgy1g24entpf59j309m04z3yc.jpg" alt></p><p>下面分别引入<strong>spark-core</strong>和<strong>spark-sql</strong>（不需要的话可以不引）运行即可。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;环境：windows 7 + idea + scala + spark&lt;/p&gt;&lt;p&gt;本地运行以后报下面错误&lt;/
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="yihao.ml/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>java.lang.ArrayIndexOutOfBoundsException 10582</title>
    <link href="yihao.ml/2019/04/16/2019-04-16-spark-idea-windows-array-index-out-of-bounds-exception/"/>
    <id>yihao.ml/2019/04/16/2019-04-16-spark-idea-windows-array-index-out-of-bounds-exception/</id>
    <published>2019-04-16T13:11:07.000Z</published>
    <updated>2019-07-02T02:01:03.768Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>环境：windows 7 + idea + scala 1.12.6 + spark 2.4.0</p><p>在IDEA中运行报下面错误</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> java.lang.ArrayIndexOutOfBoundsException: <span class="number">10582</span></span><br><span class="line">at com.thoughtworks.paranamer.BytecodeReadingParanamer$ClassReader.accept(BytecodeReadingParanamer.java:<span class="number">563</span>)</span><br><span class="line">at com.thoughtworks.paranamer.BytecodeReadingParanamer$ClassReader.access$<span class="number">200</span>(BytecodeReadingParanamer.java:<span class="number">338</span>)</span><br><span class="line">at com.thoughtworks.paranamer.BytecodeReadingParanamer.lookupParameterNames(BytecodeReadingParanamer.java:<span class="number">103</span>)</span><br><span class="line">at com.thoughtworks.paranamer.CachingParanamer.lookupParameterNames(CachingParanamer.java:<span class="number">90</span>)</span><br><span class="line">at com.fasterxml.jackson.<span class="keyword">module</span>.scala.introspect.BeanIntrospector$.getCtorParams(BeanIntrospector.scala:<span class="number">44</span>)</span><br><span class="line">at com.fasterxml.jackson.<span class="keyword">module</span>.scala.introspect.BeanIntrospector$.$anonfun$apply$<span class="number">1</span>(BeanIntrospector.scala:<span class="number">58</span>)</span><br><span class="line">at com.fasterxml.jackson.<span class="keyword">module</span>.scala.introspect.BeanIntrospector$.$anonfun$apply$<span class="number">1</span>$adapted(BeanIntrospector.scala:<span class="number">58</span>)</span><br><span class="line">at scala.collection.TraversableLike.$anonfun$flatMap$<span class="number">1</span>(TraversableLike.scala:<span class="number">241</span>)</span><br><span class="line">at scala.collection.Iterator.foreach(Iterator.scala:<span class="number">944</span>)</span><br><span class="line">at scala.collection.Iterator.foreach$(Iterator.scala:<span class="number">944</span>)</span><br><span class="line">at scala.collection.AbstractIterator.foreach(Iterator.scala:<span class="number">1432</span>)</span><br><span class="line">at scala.collection.IterableLike.foreach(IterableLike.scala:<span class="number">71</span>)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>下面是pom.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ml.yihao<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.12.6<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.4<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--&lt;dependency&gt;--&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--&lt;groupId&gt;com.thoughtworks.paranamer&lt;/groupId&gt;--&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--&lt;artifactId&gt;paranamer&lt;/artifactId&gt;--&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--&lt;version&gt;2.8&lt;/version&gt;--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--&lt;/dependency&gt;--&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                  <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                  <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                  <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font color="red">解决</font>：在pom.xml中添加paranamer即可</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.thoughtworks.paranamer<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>paranamer<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;环境：windows 7 + idea + scala 1.12.6 + spark 2.4.0&lt;/p&gt;&lt;p&gt;
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="yihao.ml/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>B-树</title>
    <link href="yihao.ml/2019/03/27/2019-03-27-B-%E6%A0%91/"/>
    <id>yihao.ml/2019/03/27/2019-03-27-B-树/</id>
    <published>2019-03-27T21:01:56.000Z</published>
    <updated>2019-07-02T02:01:03.768Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>B+树，称为B加树；那么对于B-树，谁要是读成B减树，那就太丢人了咯，它虽然带着减号，但是要读成B树。</p><p>B+树和B-树是一种基础的数据结构，做为开发人员一定要掌握。</p><h3 id="什么是b-树"><a class="markdownIt-Anchor" href="#什么是b-树"></a> 什么是B-树</h3><p>首先大家都知道数据库有索引，索引被映射成二叉索引树，被存在于磁盘之上。那么下面我们来看看为啥数据库要使用B-树？换二叉搜索树行不行？</p><p>从算法逻辑上来讲，二叉搜索树的查找速度和比较次数都是最小的，但是数据库的实现并没有用二叉搜索树，而是用了B-树和B+树，下面来说一下里面的门道。</p><p>数据库操作数据要进行频繁的“磁盘IO&quot;，因此在设计之初要充分考虑到如何优化磁盘IO造成的读写效率问题。数据库索引存于磁盘之上，当数据量比较大的时候，索引的大小可能有几个G甚至更多。当利用索引查询的时候，肯定不能将全部都加载到内存，能做的只有逐一加载每个磁盘页，这里的磁盘页对应索引树的节点。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0d03aa4913594.jpg" alt></p><p><strong>探究一下如果索引树使用二叉搜索树实现，会是一种什么样的情况，假设树的高度是4，查找的值是10</strong></p><p><img src="https://i.loli.net/2019/07/02/5d1ab0d39485597326.jpg" alt="二叉搜索树"></p><p>第1次IO<br><img src="https://i.loli.net/2019/07/02/5d1ab0d86ee3a87924.jpg" alt></p><p>第2次IO<br><img src="https://i.loli.net/2019/07/02/5d1ab0dc1ffda84614.jpg" alt></p><p>第3次IO<br><img src="https://i.loli.net/2019/07/02/5d1ab0dfa6c6715048.jpg" alt></p><p>第4次IO<br><img src="https://i.loli.net/2019/07/02/5d1ab0e339f3b63743.jpg" alt></p><blockquote><p>查找了4次命中结果，因此磁盘IO的次数是由树的高度决定。为了减少磁盘IO次数，下面使用B-树来将二叉搜索树进行“瘦身”，以此来减少IO次数！</p></blockquote><p>下面来具体介绍一下B-树（Balance Tree），一个m阶的B树具有如下几个特征：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">根结点至少有两个子女。</span><br><span class="line">每个中间节点都包含k-1个元素和k个孩子，其中 m/2 &lt;= k &lt;= m</span><br><span class="line">每一个叶子节点都包含k-1个元素，其中 m/2 &lt;= k &lt;= m</span><br><span class="line">所有的叶子结点都位于同一层。</span><br><span class="line">每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。</span><br></pre></td></tr></table></figure><p><strong>以3阶 B-树为例，来认识一下B-树的具体结构。树中的具体元素和上图二叉搜索树节点一样。</strong></p><p><img src="https://i.loli.net/2019/07/02/5d1ab0e6731ab38634.jpg" alt></p><blockquote><p>这棵树中，重点看（2,6）节点，该节点有两个元素2和6，又有三个孩子1，（3,5），8；其中1小于元素2，（3,5）在元素2,6之间，8大于（3,5），符合B-树的几个特征。</p></blockquote><p><img src="https://i.loli.net/2019/07/02/5d1ab0e9a3d7c91565.jpg" alt></p><h3 id="b-树的查找"><a class="markdownIt-Anchor" href="#b-树的查找"></a> B-树的查找</h3><p>假如要查的值为5</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0ed4084d64780.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIly1g1hopis638j30ew084jsf.jpg" alt></p><p><img src="https://i.loli.net/2019/07/02/5d1ab0f44922323956.jpg" alt></p><p><img src="https://i.loli.net/2019/07/02/5d1ab0f79956647010.jpg" alt></p><p><img src="https://i.loli.net/2019/07/02/5d1ab0fb07cbf47646.jpg" alt></p><p><img src="https://i.loli.net/2019/07/02/5d1ab0fe6963189584.jpg" alt></p><p>通过整个流程可以看出 B-树 在查询中比较次数其实不比二叉树少，尤其当单一节点中的元素数量很多时。</p><p>可是相比磁盘IO的速度，内存中比较耗时几乎可以忽略，所以只要树的高度足够低，IO次数足够少，就可以提升查找性能。</p><p>相比之下节点内部元素多一些也没有关系，仅仅是多了几次内存交互，只要不超过磁盘页的大小即可，这也是B-树的重要优势之一。</p><h3 id="b-树的插入"><a class="markdownIt-Anchor" href="#b-树的插入"></a> B-树的插入</h3><p>B-树插入新节点过程比较复杂，而且分很多种情况。这边举一个最典型例子，加入我们要插入的值是4</p><p>自顶向下查找4的节点位置，发现4应当插入到节点元素3，5之间。</p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIly1g1houyf05yj30er08aq3v.jpg" alt></p><p>节点3，5已经是两元素节点，无法再增加。父亲节点 2， 6 也是两元素节点，也无法再增加。根节点9是单元素节点，可以升级为两元素节点。于是拆分节点3，5与节点2，6，让根节点9升级为两元素节点4，9。节点6独立为根节点的第二个孩子。</p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIly1g1hov7q0bej30es06q75g.jpg" alt></p><h3 id="b-树的删除"><a class="markdownIt-Anchor" href="#b-树的删除"></a> B-树的删除</h3><p>下面演示一下B-树删除元素11的过程</p><p>自顶向下查找元素11的节点位置。</p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIly1g1hoxq0l23j30ek06p3zj.jpg" alt></p><p>删除11后，节点12只有一个孩子，不符合B树规范。因此找出12,13,15三个节点的中位数13，取代节点12，而节点12自身下移成为第一个孩子。（这个过程称为左旋）</p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIly1g1hoxpyzlrj30ed06kgmq.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/0066vfZIly1g1hoxpynidj30eu06m3zg.jpg" alt></p><h3 id="小结"><a class="markdownIt-Anchor" href="#小结"></a> 小结</h3><p>B-树主要应用于文件系统以及部分数据库索引，比如MongoDB。</p><p>大部分关系型数据库，比如myslq，则使用B+树作为索引。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;B+树，称为B加树；那么对于B-树，谁要是读成B减树，那就太丢人了咯，它虽然带着减号，但是要读成B树。&lt;/p&gt;
      
    
    </summary>
    
      <category term="算法与数据结构" scheme="yihao.ml/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
  </entry>
  
  <entry>
    <title>IDEA 无法创建 scala</title>
    <link href="yihao.ml/2019/03/21/2019-03-21-2017IDEA-%E6%97%A0%E6%B3%95%E5%88%9B%E5%BB%BA-scala/"/>
    <id>yihao.ml/2019/03/21/2019-03-21-2017IDEA-无法创建-scala/</id>
    <published>2019-03-21T15:08:38.000Z</published>
    <updated>2019-07-02T02:01:03.768Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>IDEA 无法创建Scala class</p><p>找到根目录下的 spark.iml在里面添加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">orderEntry</span> <span class="attr">type</span>=<span class="string">"library"</span> <span class="attr">name</span>=<span class="string">"scala-sdk-2.12.6"</span> <span class="attr">level</span>=<span class="string">"application"</span> /&gt;</span></span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;IDEA 无法创建Scala class&lt;/p&gt;&lt;p&gt;找到根目录下的 spark.iml在里面添加&lt;/p&gt;&lt;f
      
    
    </summary>
    
      <category term="其他" scheme="yihao.ml/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="问题总结" scheme="yihao.ml/tags/%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>ClouderaManager操作指南</title>
    <link href="yihao.ml/2019/03/19/2019-03-19-ClouderaManager%E6%89%8B%E5%86%8C/"/>
    <id>yihao.ml/2019/03/19/2019-03-19-ClouderaManager手册/</id>
    <published>2019-03-19T13:58:42.000Z</published>
    <updated>2019-07-02T02:01:03.768Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><p>作者 ： 张帅</p><h2 id="1-前言"><a class="markdownIt-Anchor" href="#1-前言"></a> 1 前言</h2><p>构建企业级别的大数据平台不是一件简单的事情，要从多个方面进行考虑，如硬件环境、软件环境。Hadoop生态圈的产品众多，部署、安装、运维、监控等工作异常琐碎，尤其当某个组件出现问题，由于机器的数量、组件的分布情况，往往会使得运维人员无从下手。</p><p>目前市面上Hadoop的发型版本主要有三种：</p><pre><code>1. Apache Hadoop2. CDH(Cloudera Distribution Hadoop)3. HDP(Hortonworks Data Platform)</code></pre><p>首先，Apache Hadoop是最正统的发行版本，版本更新快，新特性增加的多，但相对而言Bug较多，组件之间的兼容性也较差。</p><p>其次，CDH版本会将Hadoop的各个组件进行打包，形成一个发布版本，针对该版本下的各个组件进行一系列测试，补丁修复，优化策略等，保证了CDH大版本下各个Hadoop组件之间的良好协作性。大部分公司均使用该系列。</p><p>最后，HDP版本是Hortonworks公司针对Hadoop的发行版本，暂时没有过多的了解。</p><hr><p>CDH的公司Cloudera推出了Cloudera Manager用于CDH版本集群的管理。Cloudera Manager是一款管理CDH集群的端到端的应用产品，可以通过管理界面可视化的对集群中的一系列组件进行统一的管理：部署、安装、配置、监控等。</p><p>简单来说，Cloudera Manager有四大功能：</p><pre><code>1. 管理：对集群进行管理，如添加、删除节点等操作。2. 监控：监控集群的健康情况，对设置的各种指标和系统运行情况进行全面监控。3. 诊断：对集群出现的问题进行诊断，对出现的问题给出建议解决方案。4. 集成：对hadoop的多组件进行整合。</code></pre><p>Cloudera Manager的核心是Cloudera Manager Server，简称CMS。CMS提供了管理端Web界面，统一针对其他节点进行控制，如图所示</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0965fecb43192.jpg" alt></p><p><strong>组件介绍</strong></p><pre><code>1. Agent，Agent安装在集群中的各个节点上，用于启动、结束进程，安装、配置组件，监控节点等2. Management Service，由一系列的角色构成，角色有监控、预警、报告等3. Database，存储了配置和监控信息4. Cloudera Repository，Cloudera Manager用于分发的软件仓库</code></pre><h2 id="2-前置准备工作"><a class="markdownIt-Anchor" href="#2-前置准备工作"></a> 2 前置准备工作</h2><p><strong>操作系统尽量和要使用Cloudera Manager版本匹配，可以参考官方给出的Cloudera Manager与操作系统的兼容性参照表。</strong></p><p><a href="http://archive.cloudera.com/cm5" target="_blank" rel="noopener">Cloudera Manager官方下载</a></p><p><a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">Cloudera CDH 各个组件官方下载</a></p><p><a href="https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#cm_cdh_compatibility" target="_blank" rel="noopener">CDH Requirements for Cloudera Manager</a></p><h3 id="21-关于raid-0的说明"><a class="markdownIt-Anchor" href="#21-关于raid-0的说明"></a> 2.1 关于RAID 0的说明</h3><p>尽管建议采用RAID(Redundant Array of Independent Disk,即磁盘阵列)作为NameNode的存储器以保护元数据，但是若将RAID作为datanode的存储设备则不会给HDFS带来益处。HDFS所提供的节点间数据复制技术已可满足数据备份需求，无需使用RAID的冗余机制。</p><p>此外，尽管RAID条带化技术(RAID 0)被广泛用户提升性能，但是其速度仍然比用在HDFS里的JBOD(Just a Bunch Of Disks)配置慢。JBOD在所有磁盘之间循环调度HDFS块。RAID 0的读写操作受限于磁盘阵列中最慢盘片的速度，而JBOD的磁盘操作均独立，因而平均读写速度高于最慢盘片的读写速度。需要强调的是，各个磁盘的性能在实际使用中总存在相当大的差异，即使对于相同型号的磁盘。针对某一雅虎集群的评测报告<a href="http://markmail.org/message/xmzc45zi25htr7ry" target="_blank" rel="noopener">点我</a>表明，在一个测试(Gridmix)中，JBOD比RAID 0 快10%；在另一测试(HDFS写吞吐量)中，JBOD比RAID 0 快30%。</p><p>最后，若JBOD配置的某一磁盘出现故障，HDFS可以忽略该磁盘，继续工作。而RAID的某一盘片故障会导致整个磁盘阵列不可用，进而使相应节点失效。</p><p><a href="https://zh.hortonworks.com/blog/why-not-raid-0-its-about-time-and-snowflakes/" target="_blank" rel="noopener">https://zh.hortonworks.com/blog/why-not-raid-0-its-about-time-and-snowflakes/</a></p><p><em>备注：我们实际生成环境使用的是RAID5，12块4T硬盘，可用空间为40T。虽然有可能因为RAID卡的损坏导致节点故障，但是RAID卡极大程度的提高了IO性能，并且在逻辑上将12块硬盘映射为了一块大磁盘。单块磁盘故障不会影响到节点的状态。</em></p><h3 id="22-ip地址"><a class="markdownIt-Anchor" href="#22-ip地址"></a> 2.2 IP地址</h3><p>尽可能的将集群部署在同一网段中，避免夸路由进行数据交互。</p><h3 id="23-主机名及映射"><a class="markdownIt-Anchor" href="#23-主机名及映射"></a> 2.3 主机名及映射</h3><p>主机名映射为全限定名称和短名称的形式，例如：<br>​<br>​192.168.15.193 <a href="http://datacenter01.aisino.com" target="_blank" rel="noopener">datacenter01.aisino.com</a> datacenter01</p><h3 id="24-启动级别"><a class="markdownIt-Anchor" href="#24-启动级别"></a> 2.4 启动级别</h3><p>启动级别尽可能设置为3，避免其他图形界面消耗机器资源。</p><h3 id="25-防火墙和selinux"><a class="markdownIt-Anchor" href="#25-防火墙和selinux"></a> 2.5 防火墙和selinux</h3><p>关闭防火墙的原因是一些动态任务（YARN、Spark Executor）在运行时动态分配端口号，如果开启防火墙的话会导致任务无法连接，导致任务执行失败。</p><p>selinux记得要关闭。</p><h3 id="26-配置系统文件打开数量以及用户最大进程数量"><a class="markdownIt-Anchor" href="#26-配置系统文件打开数量以及用户最大进程数量"></a> 2.6 配置系统文件打开数量以及用户最大进程数量</h3><pre><code>vi /etc/security/limits.conf* soft nofile 65536* hard nofile 65536* soft nproc 16384* hard nproc 16384</code></pre><p><em>注意，星号表示所有用户</em></p><h3 id="27-配置ntp服务"><a class="markdownIt-Anchor" href="#27-配置ntp服务"></a> 2.7 配置NTP服务</h3><p>Cloudera Manager要求各个节点都启动NTP服务，保证集群内各节点之间的时间同步，由于是分布式架构，节点与节点之间时刻保持通信，如果节点之间时间差别过大，会导致通信故障，从而节点造成宕机。</p><h3 id="28-配置ssh"><a class="markdownIt-Anchor" href="#28-配置ssh"></a> 2.8 配置SSH</h3><p>所有节点都需要配置SSH免密登录（包括自己）。</p><h2 id="3-安装cloudera-manager"><a class="markdownIt-Anchor" href="#3-安装cloudera-manager"></a> 3 安装Cloudera Manager</h2><p><strong>本次安装环境为CDH-5.7.0版本</strong></p><h3 id="31-下载安装包"><a class="markdownIt-Anchor" href="#31-下载安装包"></a> 3.1 下载安装包</h3><pre><code>1. 下载Cloudera Manager安装包http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.7.0_x86_64.tar.gz2. 下载Parcel离线包http://archive.cloudera.com/cdh5/parcels/5.7.0/CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcelhttp://archive.cloudera.com/cdh5/parcels/5.7.0/CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha1http://archive.cloudera.com/cdh5/parcels/5.7.0/manifest.json</code></pre><h3 id="32-安装yum依赖"><a class="markdownIt-Anchor" href="#32-安装yum依赖"></a> 3.2 安装yum依赖</h3><pre><code>yum install -y chkconfig python bind-utils psmisc libxslt zlib sqliteyum install -y cyrus-sasl-plain cyrus-sasl-gssapi fuse portmap fuse-libsyum install -y redhat-lsb bind-utils libxsltyum install -y protobuf snappy</code></pre><h3 id="33-安装mysql数据库"><a class="markdownIt-Anchor" href="#33-安装mysql数据库"></a> 3.3 安装MySQL数据库</h3><p>Cloudera Manager需要将配置信息以及监控信息存储至数据库中，这里采用MySQL数据。</p><h3 id="34-mysql驱动包路径"><a class="markdownIt-Anchor" href="#34-mysql驱动包路径"></a> 3.4 MySQL驱动包路径</h3><p>必须将MySQL驱动包复制至/usr/share/java目录下，并且驱动包的名称必须为mysql-connector-java.jar。</p><p><strong>注意，所有节点都需要MySQL驱动包</strong></p><h3 id="35-在mysql中创建数据库"><a class="markdownIt-Anchor" href="#35-在mysql中创建数据库"></a> 3.5 在MySQL中创建数据库</h3><pre><code>create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database cloudera DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</code></pre><h3 id="36-解压cloudera-manager安装包"><a class="markdownIt-Anchor" href="#36-解压cloudera-manager安装包"></a> 3.6 解压Cloudera Manager安装包</h3><p>Cloudera Manager Server服务安装在Manager1服务器上，因此需要将Cloudera Manager的压缩包以及所有Parcel文件上传至服务器，并将Cloudera Manager压缩包解压至/opt/cloudera-manager目录下</p><h3 id="37-创建用户"><a class="markdownIt-Anchor" href="#37-创建用户"></a> 3.7 创建用户</h3><p>在所有节点上创建cloudera-scm用户：<br>​<br>​useradd --system --home=/opt/cloudera-manager/cm-5.7.0/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment “Cloudera SCM User” cloudera-scm</p><h3 id="38-创建cloudera-manager-server元数据目录"><a class="markdownIt-Anchor" href="#38-创建cloudera-manager-server元数据目录"></a> 3.8 创建Cloudera Manager Server元数据目录</h3><p>在主节点上需要创建Cloudera Manager Server的元数据目录：</p><pre><code>mkdir /var/cloudera-scm-serverchown cloudera-scm:cloudera-scm /var/cloudera-scm-serverchown cloudera-scm:cloudera-scm /opt/cloudera-manager</code></pre><h3 id="39-复制cloudera-manager目录到其他节点"><a class="markdownIt-Anchor" href="#39-复制cloudera-manager目录到其他节点"></a> 3.9 复制cloudera-manager目录到其他节点</h3><p>首先将/opt/cloudera-manager/cm-5.7.0/etc/cloudera-scm-agent/config.ini文件中server_host配置项的地址更改为主节点的地址</p><p>将主节点上/opt/cloudera-manager目录复制至其他集群节点下，命令如下：</p><pre><code>scp -r /opt/cloudera-manager 主机:/opt/</code></pre><h3 id="310-创建parcel目录"><a class="markdownIt-Anchor" href="#310-创建parcel目录"></a> 3.10 创建Parcel目录</h3><p>在主节点上创建Parcel包的存储目录，命令如下：</p><pre><code>mkdir -p /opt/cloudera/parcel-repochown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo</code></pre><p>将以下文件复制到该目录下</p><ul><li>CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel</li><li>CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha1</li><li>manifest.json</li></ul><p>将<strong>CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha1</strong>文件的名称修改为<strong>CDH-5.7.0-1.cdh5.7.0.p0.45-el7.parcel.sha</strong></p><h3 id="311-创建parcel包分发目录"><a class="markdownIt-Anchor" href="#311-创建parcel包分发目录"></a> 3.11 创建Parcel包分发目录</h3><p>Cloudera Manager在安装的过程中，需要将安装包复制到集群的各个节点上，因此需要在集群各个节点上事先创建Parcel包的分发目录。</p><p>在所有节点上：</p><pre><code>mkdir -p /opt/cloudera/parcelschown cloudera-scm:cloudera-scm /opt/cloudera/parcels</code></pre><h3 id="312-初始化csm数据库脚本"><a class="markdownIt-Anchor" href="#312-初始化csm数据库脚本"></a> 3.12 初始化CSM数据库脚本</h3><p>在主节点上，执行初始化脚本：</p><pre><code>/opt/cloudera-manager/cm-5.7.0/share/cmf/schema/scm_prepare_database.sh mysql -hmanager1 -uroot -p123456 --scm-host manager1 scmdbn scmdbu scmdbp</code></pre><p>说明：这个脚本就是用来创建和配置CMS需要的数据库的脚本。</p><p>各参数是指：</p><p>mysql：数据库用的是mysql，如果安装过程中用的oracle，那么该参数就应该改为oracle。</p><p>-hmanager1：数据库建立在manager1主机上面。也就是主节点上面。</p><p>-uroot：root身份运行mysql。</p><p>-123456：mysql的root密码是123456。</p><p>–scm-host manager1：CMS的主机，一般是和mysql安装的主机是在同一个主机上。</p><p>最后三个参数是：数据库名，数据库用户名，数据库密码。</p><p><strong>我的实际环境中采用如下命令</strong></p><pre><code>/opt/cloudera-manager/cm-5.7.0/share/cmf/schema/scm_prepare_database.sh mysql -hlocalhost -uroot -p -P13066 --scm-host localhost cloudera root BIM@123%$#qwe </code></pre><h3 id="313-配置与启动cloudera-manager-server"><a class="markdownIt-Anchor" href="#313-配置与启动cloudera-manager-server"></a> 3.13 配置与启动Cloudera Manager Server</h3><p>首先将Cloudera Manager Server的启动脚本复制到/etc/init.d/目录下：<br>​<br>​cp<br>​/opt/cloudera-manager/cm-5.7.0/etc/init.d/cloudera-scm-server<br>​/etc/init.d/cloudera-scm-server</p><p>配置/etc/init.d/cloudera-scm-server：</p><pre><code>vi /etc/init.d/cloudera-scm-server找到CMF_DEFAULTS配置项，进行修改。CMF_DEFAULTS=${CMF_DEFAULTS:-/opt/cloudera-manager/cm-5.7.0/etc/default}</code></pre><p>启动Cloudera Manager Server：</p><pre><code>service cloudera-scm-server startchkconfig cloudera-scm-server on</code></pre><h3 id="314-配置与启动cloudera-manager-agent"><a class="markdownIt-Anchor" href="#314-配置与启动cloudera-manager-agent"></a> 3.14 配置与启动Cloudera Manager Agent</h3><p>在所有节点上创建agent的运行时目录：</p><pre><code>mkdir /opt/cloudera-manager/cm-5.7.0/run/cloudera-scm-agent</code></pre><p>将Cloudera Manager Agent的启动脚本复制到/etc/init.d/目录下：</p><pre><code>cp \/opt/cloudera-manager/cm-5.7.0/etc/init.d/cloudera-scm-agent \/etc/init.d/cloudera-scm-agent</code></pre><p>配置Cloudera Manager Agent：</p><pre><code>vi /etc/init.d/cloudera-scm-agent 找到CMF_DEFAULTS配置项，进行修改。CMF_DEFAULTS=${CMF_DEFAULTS:-/opt/cloudera-manager/cm-5.7.0/etc/default}</code></pre><p>启动Cloudera Manager Agent：</p><pre><code>service cloudera-scm-agent startchkconfig cloudera-scm-agent on</code></pre><h3 id="315-日志文件路径"><a class="markdownIt-Anchor" href="#315-日志文件路径"></a> 3.15 日志文件路径</h3><p>启动Server或Agent由于各种原因可能会导致启动失败，因此需要查看日志文件定位错误信息，进行修复。</p><p>Server的日志文件位于：</p><pre><code>/opt/cloudera-manager/cm-5.7.0/log/cloudera-scm-server</code></pre><p>Agent的日志文件位于：</p><pre><code>/opt/cloudera-manager/cm-5.7.0/log/cloudera-scm-agent</code></pre><h3 id="316-进入cloudera-manager-server管理页面"><a class="markdownIt-Anchor" href="#316-进入cloudera-manager-server管理页面"></a> 3.16 进入Cloudera Manager Server管理页面</h3><p>当Server与Agent全部启动完成后，可以访问CMS的WEB管理页面<a href="http://localhost:7180" title="例子" target="_blank" rel="noopener">example link</a>，如图所示。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab09925de940367.jpg" alt></p><p><em>账户密码均为admin</em></p><h2 id="4-安装cdh"><a class="markdownIt-Anchor" href="#4-安装cdh"></a> 4 安装CDH</h2><p><em>在Cloudera Manager Server的WEB管理页面中，可以批量进行组件的安装、配置等，下面开始安装CDH各种组件。</em></p><h3 id="41-接受协议"><a class="markdownIt-Anchor" href="#41-接受协议"></a> 4.1 接受协议</h3><p>首次登陆时，会自动弹出“接受协议”页面，接受即可。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab09f809b038392.jpg" alt></p><h3 id="42-选择版本"><a class="markdownIt-Anchor" href="#42-选择版本"></a> 4.2 选择版本</h3><p>选择版本时，选择免费版本即可。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0a42f11c22720.jpg" alt></p><h3 id="43-查看当前已管理主机"><a class="markdownIt-Anchor" href="#43-查看当前已管理主机"></a> 4.3 查看当前已管理主机</h3><p>如果Agent正产启动，在“当前管理的主机”页面会显示所有管理主机，否则请检查Agent是否正常启动。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0a96fb1b57373.jpg" alt></p><p>选择所有主机，点击“继续”按钮。</p><h3 id="44-等待系统分发parcel包"><a class="markdownIt-Anchor" href="#44-等待系统分发parcel包"></a> 4.4 等待系统分发Parcel包</h3><p>Cloudera Manager会将Parcel包分发至各个节点，等待几分钟即可。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0ada696c71741.jpg" alt></p><p><strong>注意，在分发Parcel包的过程中，可以会出现分发失败的问题，查看相应的Agent日志，定位错误，进行修复。</strong></p><p><strong>比较常见的一个错误是Python脚本出现问题，需要进行一些修改，参见链接：</strong><br><a href="http://www.jianshu.com/p/0d70a67b66b2" target="_blank" rel="noopener">http://www.jianshu.com/p/0d70a67b66b2</a></p><h4 id="45-等待检查主机正确性"><a class="markdownIt-Anchor" href="#45-等待检查主机正确性"></a> 4.5 等待检查主机正确性</h4><p>Parcel包分发完成后，Cloudera Manager会检查主机的的正确性，包括一些优化的配置、要关闭的属性等，该步骤非常重要，一定要根据检查结果对主机进行配置修复，否则在以后的过程中会出现各种不可预料问题。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0b07998314628.jpg" alt></p><h3 id="46-选择要安装的组件"><a class="markdownIt-Anchor" href="#46-选择要安装的组件"></a> 4.6 选择要安装的组件</h3><p>主机正确性检查完成后，就可以进入安装环节了，Cloudera Manager会要求你选择要安装的组件。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0b607b8288865.jpg" alt></p><p><strong>选择“自定义服务”，选择集群中要安装的组件。</strong></p><p><img src="https://i.loli.net/2019/07/02/5d1ab0be6732e32844.jpg" alt></p><h3 id="47-设置数据库"><a class="markdownIt-Anchor" href="#47-设置数据库"></a> 4.7 设置数据库</h3><p>选择完成要安装的组件后，需要为Cloudera Manager配置运行时数据库环境，如图所示。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0c3709f785092.jpg" alt></p><p>配置的数据库在前面的步骤中已经提前创建完成。</p><h3 id="48-设置组件的基本运行环境"><a class="markdownIt-Anchor" href="#48-设置组件的基本运行环境"></a> 4.8 设置组件的基本运行环境</h3><p>该步骤主要用于设置组件的基本运行环境，例如NameNode的节点、DataNode的节点、数据存放的目录等，该步骤根据实际的物理环境进行设置即可。</p><h3 id="49-等待启动集群"><a class="markdownIt-Anchor" href="#49-等待启动集群"></a> 4.9 等待启动集群</h3><p>在集群的启动过程中，可能会因为权限或其他问题导致某些服务启动失败，只需要根据错误信息进行修复，即可启动完成。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0c78f62f95988.jpg" alt></p><h3 id="410-启动成功"><a class="markdownIt-Anchor" href="#410-启动成功"></a> 4.10 启动成功</h3><p>集群启动成功后，可在首页查看集群的总览。</p><p><img src="https://i.loli.net/2019/07/02/5d1ab0cd36cd412231.jpg" alt></p><h3 id="411-其他"><a class="markdownIt-Anchor" href="#411-其他"></a> 4.11 其他</h3><p>后续可以启动HDFS的HA、YARN的HA以及优化各个组件的配置等。</p><p>感谢张帅分享。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;p&gt;作者 ： 张帅&lt;/p&gt;&lt;h2 id=&quot;1-前言&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; h
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="CDH" scheme="yihao.ml/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>用户行为指分析</title>
    <link href="yihao.ml/2019/03/15/2019-03-15-%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%8C%87%E5%88%86%E6%9E%90/"/>
    <id>yihao.ml/2019/03/15/2019-03-15-用户行为指分析/</id>
    <published>2019-03-15T14:24:40.000Z</published>
    <updated>2019-07-02T02:01:03.768Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --><h3 id="1-概述"><a class="markdownIt-Anchor" href="#1-概述"></a> 1. 概述</h3><p>构件企业级别的大数据平台并不是一件容易的事情，起步阶段要想的全面一些，从多个方面进行考虑。例如关于硬件环境？Hadoop生态圈的产品众多，到底要选择那些组件？另外部署、安装、运维、监控等工作异常繁琐，怎么解决后期管理问题？针对这些问题，下面从软硬件、后期运维和业务的角度来阐述详细规划。</p><h3 id="2-软件支持"><a class="markdownIt-Anchor" href="#2-软件支持"></a> 2. 软件支持</h3><p>1） 首先我们说说关于<em>Hadoop</em>的发行版，目前<em>Hadoop</em>的发行版主要有三种，我们怎么选?</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. Apahce 基金会 Hadoop</span><br><span class="line">2. CDH (Cloudera Distribution Hadoop)</span><br><span class="line">3. HDP （Hortonworks Data Platform）</span><br></pre></td></tr></table></figure><ul><li><p>Hadoop属于Apache基金会的顶级项目，<em>Apahce</em>版本迭代速度很快，新特性很多，相对的bug就会很多，组件与组件之间的兼容性也会很差。在实际中大部分它的使用者多为进行学术研究，喜欢爱折腾的那号人，所以这个版本不首选。</p></li><li><p>Cloudera 针对<em>Apahce Hadoop</em>在每个大版本变化后进行一次打包，很好的解决了系统不稳定的问题。与此同时，Cloudera公司推出的<em>Cloudera Manager</em>版本集群管理工具，它可以完成一键部署集群，一键拓展，监控，自动诊断等操作。最重要的是CDH的文档很丰富，解决问题的速度也会很高。目前知晓的使用此发行版的公司有：360、东方航空等，这个版本是目前<strong>首选</strong>。</p></li><li><p>HDP版本是<em>Hortonworks</em>公司针对<em>Hadoop</em>的发行版本，目前也是比较小众的一个，现阶段还处于探索发展阶段，所以不首选。</p></li></ul><p>有了自动化部署Cloudera Manager ,后期运维就能轻松的多，而且里面的自动诊断功能也是相当nice！</p><p>2） 关于具体应用组件Flume、Hadoop、Kafka、Hive、Spark、Redis、Mysql、Sqlserver、Sqoop、Hue、Oozie</p><h3 id="3-硬件支持"><a class="markdownIt-Anchor" href="#3-硬件支持"></a> 3. 硬件支持</h3><p>配置Hadoop集群至少要求有三台Server，一台为主服务器，三台为从服务器。主服务器上会跑大量后台进程，所以主服务器的配置要远远优于从服务器。</p><p>**主服务器 ** 最少1台</p><table><thead><tr><th style="text-align:center">硬件类型</th><th style="text-align:center">要求</th></tr></thead><tbody><tr><td style="text-align:center">OS</td><td style="text-align:center">linux</td></tr><tr><td style="text-align:center">硬盘</td><td style="text-align:center">1~4TB</td></tr><tr><td style="text-align:center">CPU</td><td style="text-align:center">2个频率为2~2.5GHz的四核或六核</td></tr><tr><td style="text-align:center">内存</td><td style="text-align:center">16~32GB</td></tr></tbody></table><p>**从服务器 **最少两台</p><table><thead><tr><th style="text-align:center">硬件类型</th><th style="text-align:center">要求</th></tr></thead><tbody><tr><td style="text-align:center">OS</td><td style="text-align:center">linux</td></tr><tr><td style="text-align:center">硬盘</td><td style="text-align:center">1~2TB</td></tr><tr><td style="text-align:center">CPU</td><td style="text-align:center">2个频率为2~2.5GHz的四核或六核</td></tr><tr><td style="text-align:center">内存</td><td style="text-align:center">4~16GB</td></tr></tbody></table><h3 id="4-如何进行网站流量分析"><a class="markdownIt-Anchor" href="#4-如何进行网站流量分析"></a> 4. 如何进行网站流量分析</h3><p>下面对平台初期统计指标做一个梳理，另外会介绍一下统计指标的方向。</p><h3 id="41-指标举例"><a class="markdownIt-Anchor" href="#41-指标举例"></a> 4.1 指标举例</h3><p>需求：今日，昨天，前天 所有来访者，平均请求的页面数</p><p>需求：按照来源及时间维度统计PVS，并按照PV大小倒序排序</p><p>需求：按照时间维度，比如，统计一天内各小时产生最多pvs的来源topN</p><p>需求：统计每日最热门的功能top10</p><p>需求：按照时间维度比如小时来统计独立访客及其产生的pv</p><p>需求：将每天的新访客统计出来</p><p>需求：查询今日所有回头访客及其访问次数</p><p>需求：统计出每天所有用户访问网站的平均次数</p><p>需求：回头/单次访客的访问比重，比如当日回头客占比</p><p>需求：人均访问频度</p><p>需求：漏斗模型统计，以模型上传业务来评估模型转化设计的合理性。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">step1: 开启本地化组建</span><br><span class="line">step2: 选择模型、选择目录</span><br><span class="line">step3: 设置各种参数，勾选模型对比（或者是其他操作）</span><br><span class="line">step4: 进行模型转换</span><br><span class="line">step5: 进行模型上传</span><br><span class="line">step6: 模型浏览</span><br><span class="line"></span><br><span class="line">六步为一个业务指标，我们可以拓展下面业务指标</span><br><span class="line">**1) 查询每一个步骤的总访问人数**</span><br><span class="line">**2) 查询每一步骤相对于路径起点人数的比例**</span><br><span class="line">**3) 查询每一步骤相对于上一步骤的漏出率**</span><br></pre></td></tr></table></figure><p>​</p><p>除了上面一些指标以外，可以从下面方向入手，发掘一些有价值的数据报表。部分参考**《网站分析实战——如何以数据驱动决策，提升网站价值》** <em>王彦平，吴盛锋编著</em></p><h3 id="42-关于统计统计方向"><a class="markdownIt-Anchor" href="#42-关于统计统计方向"></a> 4.2 关于统计统计方向</h3><p><strong>1） 基础分析（PV,IP,UV）</strong></p><p><strong>趋势分析</strong>：根据选定的时段，提供网站流量数据，通过流量趋势变化形态，为您分析网站访客的访问规律、网站发展状况提供参考。</p><p><strong>对比分析</strong>：根据选定的两个对比时段，提供网站流量在时间上的纵向对比报表，帮您发现网站发展状况、发展规律、流量变化率等。</p><p><strong>当前在线</strong>：提供当前时刻站点上的访客量，以及最近15分钟流量、来源、受访、访客变化情况等，方便用户及时了解当前网站流量状况。</p><p><strong>访问明细</strong>：提供最近7日的访客访问记录，可按每个PV或每次访问行为（访客的每次会话）显示，并可按照来源、搜索词等条件进行筛选。 通过访问明细，用户可以详细了解网站流量的累计过程，从而为用户快速找出流量变动原因提供最原始、最准确的依据。</p><hr><p><strong>2)来源分析</strong></p><p><strong>来源分类</strong>：提供不同来源形式（直接输入、搜索引擎、其他外部链接、站内来源）、不同来源项引入流量的比例情况。通过精确的量化数据，帮助用户分析什么类型的来路产生的流量多、效果好，进而合理优化推广方案。</p><p><strong>搜索引擎</strong>：提供各搜索引擎以及搜索引擎子产品引入流量的比例情况。从搜索引擎引入流量的的角度，帮助用户了解网站的SEO、SEM效果，从而为制定下一步SEO、SEM计划提供依据。</p><p><strong>搜索词</strong>：提供访客通过搜索引擎进入网站所使用的搜索词，以及各搜索词引入流量的特征和分布。帮助用户了解各搜索词引入流量的质量，进而了解访客的兴趣关注点、网站与访客兴趣点的匹配度，为优化SEO方案及SEM提词方案提供详细依据。</p><p><strong>最近7日的访客搜索记录</strong>，可按每个PV或每次访问行为（访客的每次会话）显示，并可按照访客类型、地区等条件进行筛选。为您搜索引擎优化提供最详细的原始数据。</p><p><strong>来路域名</strong>：提供具体来路域名引入流量的分布情况，并可按“社会化媒体”、“搜索引擎”、“邮箱”等网站类型对来源域名进行分类。 帮助用户了解哪类推广渠道产生的流量多、效果好，进而合理优化网站推广方案。</p><p><strong>来路页面</strong>：提供具体来路页面引入流量的分布情况。 尤其对于通过流量置换、包广告位等方式从其他网站引入流量的用户，该功能可以方便、清晰地展现广告引入的流量及效果，为优化推广方案提供依据。</p><p><strong>来源升降榜</strong>：提供开通统计后任意两日的TOP10000搜索词、来路域名引入流量的对比情况，并按照变化的剧烈程度提供排行榜。 用户可通过此功能快速找到哪些来路对网站流量的影响比较大，从而及时排查相应来路问题。</p><hr><p><strong>3) 受访分析</strong></p><p><strong>受访域名</strong>：提供访客对网站中各个域名的访问情况。 一般情况下，网站不同域名提供的产品、内容各有差异，通过此功能用户可以了解不同内容的受欢迎程度以及网站运营成效。</p><p><strong>受访页面</strong>：提供访客对网站中各个页面的访问情况。 站内入口页面为访客进入网站时浏览的第一个页面，如果入口页面的跳出率较高则需要关注并优化；站内出口页面为访客访问网站的最后一个页面，对于离开率较高的页面需要关注并优化。</p><p><strong>受访升降榜</strong>：提供开通统计后任意两日的TOP10000受访页面的浏览情况对比，并按照变化的剧烈程度提供排行榜。 可通过此功能验证经过改版的页面是否有流量提升或哪些页面有巨大流量波动，从而及时排查相应问题。</p><p><strong>热点图</strong>：记录访客在页面上的鼠标点击行为，通过颜色区分不同区域的点击热度；支持将一组页面设置为&quot;关注范围&quot;，并可按来路细分点击热度。 通过访客在页面上的点击量统计，可以了解页面设计是否合理、广告位的安排能否获取更多佣金等。</p><p><strong>用户视点</strong>：提供受访页面对页面上链接的其他站内页面的输出流量，并通过输出流量的高低绘制热度图，与热点图不同的是，所有记录都是实际打开了下一页面产生了浏览次数（PV）的数据，而不仅仅是拥有鼠标点击行为。</p><p><strong>访问轨迹</strong>：提供观察焦点页面的上下游页面，了解访客从哪些途径进入页面，又流向了哪里。 通过上游页面列表比较出不同流量引入渠道的效果；通过下游页面列表了解用户的浏览习惯，哪些页面元素、内容更吸引访客点击。</p><hr><p><strong>4) 访客分析</strong></p><p><strong>地区运营商</strong>：提供各地区访客、各网络运营商访客的访问情况分布。 地方网站、下载站等与地域性、网络链路等结合较为紧密的网站，可以参考此功能数据，合理优化推广运营方案。</p><p><strong>终端详情</strong>：提供网站访客所使用的浏览终端的配置情况。 参考此数据进行网页设计、开发，可更好地提高网站兼容性，以达到良好的用户交互体验。</p><p><strong>新老访客</strong>：当日访客中，历史上第一次访问该网站的访客记为当日新访客；历史上已经访问过该网站的访客记为老访客。 新访客与老访客进入网站的途径和浏览行为往往存在差异。该功能可以辅助分析不同访客的行为习惯，针对不同访客优化网站，例如为制作新手导航提供数据支持等。</p><p><strong>忠诚度</strong>：从访客一天内回访网站的次数（日访问频度）与访客上次访问网站的时间两个角度，分析访客对网站的访问粘性、忠诚度、吸引程度。 由于提升网站内容的更新频率、增强用户体验与用户价值可以有更高的忠诚度，因此该功能在网站内容更新及用户体验方面提供了重要参考。</p><p><strong>活跃度</strong>：从访客单次访问浏览网站的时间与网页数两个角度，分析访客在网站上的活跃程度。 由于提升网站内容的质量与数量可以获得更高的活跃度，因此该功能是网站内容分析的关键指标之一。</p><hr><p><strong>5) 转化路径分析</strong></p><p><em>转化定义:访客在您的网站完成了某项您期望的活动，记为一次转化，如注册或下载。</em></p><p>目标示例</p><ul><li><p>获得用户目标：在线注册、创建账号等。</p></li><li><p>咨询目标：咨询、留言、电话等。</p></li><li><p>互动目标：模型转化、模型分享等。</p></li><li><p>收入目标：购买简约版、付款等。</p></li></ul><p>转化数据的应用</p><ul><li><p>在报告的自定义指标中勾选转化指标，实时掌握网站的推广及运营情况。</p></li><li><p>结合“全部来源”、“转化路径”、“页面上下游”等报告分析访问漏斗，提高转化率。</p></li><li><p>对“转化目标”设置价值，预估转化收益，衡量ROI。</p></li></ul><p><strong>路径分析</strong>：根据设置的特定路线，监测某一流程的完成转化情况，算出每步的转换率和流失率数据，如注册流程，购买流程等。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jul 02 2019 02:01:20 GMT+0000 (Coordinated Universal Time) --&gt;&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#1-概述&quot;&gt;&lt;/
      
    
    </summary>
    
      <category term="大数据" scheme="yihao.ml/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
</feed>
