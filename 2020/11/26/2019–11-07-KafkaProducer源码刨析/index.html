<!-- build time:Thu Nov 26 2020 12:31:12 GMT+0000 (GMT) --><!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>Kafka源码刨析之Produer发送模型（一） | zyh</title><meta name="description" content="kafka是一个分布式的消息中间件，目前应用十分广泛。看源码不仅可以了解其底层的细节，同时，在看代码时，也能跟着大神们学到很多的编程技巧。 KafkaProducer的使用在Kafka中，Client端是由Java实现的，Server端是Scala实现的。下面我们从Client端开始，分析一下Kafaka中的Producer模型。开始之前我们先看一下怎么向Topic中生产数据。1234567891"><meta name="keywords" content="kafka"><meta property="og:type" content="article"><meta property="og:title" content="Kafka源码刨析之Produer发送模型（一）"><meta property="og:url" content="yihao.ml/2020/11/26/2019–11-07-KafkaProducer源码刨析/index.html"><meta property="og:site_name" content="YIHAO&#39;S BLOG"><meta property="og:description" content="kafka是一个分布式的消息中间件，目前应用十分广泛。看源码不仅可以了解其底层的细节，同时，在看代码时，也能跟着大神们学到很多的编程技巧。 KafkaProducer的使用在Kafka中，Client端是由Java实现的，Server端是Scala实现的。下面我们从Client端开始，分析一下Kafaka中的Producer模型。开始之前我们先看一下怎么向Topic中生产数据。1234567891"><meta property="og:locale" content="default"><meta property="og:image" content="https://i.loli.net/2019/11/07/r4CtIEAcmhwN7xo.jpg"><meta property="og:image" content="https://i.loli.net/2019/11/07/bL3CzBcH7G1f2Pe.jpg"><meta property="og:updated_time" content="2020-11-26T12:30:41.019Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Kafka源码刨析之Produer发送模型（一）"><meta name="twitter:description" content="kafka是一个分布式的消息中间件，目前应用十分广泛。看源码不仅可以了解其底层的细节，同时，在看代码时，也能跟着大神们学到很多的编程技巧。 KafkaProducer的使用在Kafka中，Client端是由Java实现的，Server端是Scala实现的。下面我们从Client端开始，分析一下Kafaka中的Producer模型。开始之前我们先看一下怎么向Topic中生产数据。1234567891"><meta name="twitter:image" content="https://i.loli.net/2019/11/07/r4CtIEAcmhwN7xo.jpg"><link rel="canonical" href="yihao.ml/2020/11/26/2019–11-07-KafkaProducer源码刨析/index.html"><link rel="alternate" href="/atom.xml" title="YIHAO&#39;S BLOG" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1.4.0/dist/gitalk.min.css"></head><body class="main-center" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/Demo233" target="_blank"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">zyh</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shanghai, China</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="Search"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">Home</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">Archives</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">Categories</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">Tags</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">Repository</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">Links</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">About</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/Demo233" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/5592904662/home" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/cnnqjban521" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.facebook.com/yihao.zhao.7" target="_blank" title="Facebook" data-toggle="tooltip" data-placement="top"><i class="icon icon-facebook"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">Board</h3><div class="widget-body"><div id="board"><div class="content"><p>兴趣与事业的一致以及婚姻与爱情的一致</p></div></div></div></div><div class="widget"><h3 class="widget-title">Categories</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Autodesk/">Autodesk</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/其他/">其他</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/分布式/">分布式</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/前端/">前端</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/后端/">后端</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">46</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/微服务/">微服务</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/游戏开发/">游戏开发</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/物联网/">物联网</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法与数据结构/">算法与数据结构</a><span class="category-list-count">23</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程语言/">编程语言</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/自动化测试/">自动化测试</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/转载/">转载</a><span class="category-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">Tags</h3><div class="widget-body"><ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ActiveMQ/">ActiveMQ</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Anaconda/">Anaconda</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Azkaban/">Azkaban</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CDH/">CDH</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DataTable/">DataTable</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/">Docker</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Editor/">Editor</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FFmpeg/">FFmpeg</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FTP/">FTP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flink/">Flink</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flume/">Flume</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Forge/">Forge</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/">Git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/">HBase</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HDFS/">HDFS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/">Hadoop</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hbase/">Hbase</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hibernate/">Hibernate</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/">Hive</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hue/">Hue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/InfluxDB/">InfluxDB</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JVM/">JVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jekyll/">Jekyll</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/">Kafka</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MapReduce/">MapReduce</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MatLab/">MatLab</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Maven/">Maven</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minecraft/">Minecraft</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mybatis/">Mybatis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIO/">NIO</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NodeJS/">NodeJS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python3/">Python3</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/">R</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RPC/">RPC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Raspberry-Pi/">Raspberry Pi</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/">Redis</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scala/">Scala</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Selenium/">Selenium</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Shiro/">Shiro</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring/">Spring</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Storm/">Storm</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Struts2/">Struts2</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">10</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vue/">Vue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WebSocket/">WebSocket</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Wow/">Wow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zookeeper/">Zookeeper</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ceph/">ceph</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k-means/">k-means</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/">kafka</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zTree/">zTree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/图/">图</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/递归/">递归</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/问题总结/">问题总结</a><span class="tag-list-count">13</span></li></ul></div></div><div class="widget"><h3 class="widget-title">Tag Cloud</h3><div class="widget-body tagcloud"><a href="/tags/ActiveMQ/" style="font-size:13px">ActiveMQ</a> <a href="/tags/Anaconda/" style="font-size:13px">Anaconda</a> <a href="/tags/Azkaban/" style="font-size:13px">Azkaban</a> <a href="/tags/C/" style="font-size:13px">C</a> <a href="/tags/CDH/" style="font-size:13.67px">CDH</a> <a href="/tags/DataTable/" style="font-size:13px">DataTable</a> <a href="/tags/Docker/" style="font-size:13.33px">Docker</a> <a href="/tags/Editor/" style="font-size:13px">Editor</a> <a href="/tags/FFmpeg/" style="font-size:13.17px">FFmpeg</a> <a href="/tags/FTP/" style="font-size:13px">FTP</a> <a href="/tags/Flink/" style="font-size:13px">Flink</a> <a href="/tags/Flume/" style="font-size:13.17px">Flume</a> <a href="/tags/Forge/" style="font-size:13px">Forge</a> <a href="/tags/Git/" style="font-size:13.17px">Git</a> <a href="/tags/HBase/" style="font-size:13px">HBase</a> <a href="/tags/HDFS/" style="font-size:13px">HDFS</a> <a href="/tags/Hadoop/" style="font-size:13.5px">Hadoop</a> <a href="/tags/Hbase/" style="font-size:13.33px">Hbase</a> <a href="/tags/Hibernate/" style="font-size:13px">Hibernate</a> <a href="/tags/Hive/" style="font-size:13.17px">Hive</a> <a href="/tags/Hue/" style="font-size:13px">Hue</a> <a href="/tags/InfluxDB/" style="font-size:13px">InfluxDB</a> <a href="/tags/JVM/" style="font-size:13px">JVM</a> <a href="/tags/Java/" style="font-size:13px">Java</a> <a href="/tags/Jekyll/" style="font-size:13.17px">Jekyll</a> <a href="/tags/Kafka/" style="font-size:13.17px">Kafka</a> <a href="/tags/Linux/" style="font-size:13px">Linux</a> <a href="/tags/MapReduce/" style="font-size:13.5px">MapReduce</a> <a href="/tags/MatLab/" style="font-size:13.17px">MatLab</a> <a href="/tags/Maven/" style="font-size:13.17px">Maven</a> <a href="/tags/Minecraft/" style="font-size:13px">Minecraft</a> <a href="/tags/Mybatis/" style="font-size:13px">Mybatis</a> <a href="/tags/NIO/" style="font-size:13px">NIO</a> <a href="/tags/NodeJS/" style="font-size:13px">NodeJS</a> <a href="/tags/Python3/" style="font-size:13.33px">Python3</a> <a href="/tags/R/" style="font-size:13.17px">R</a> <a href="/tags/RPC/" style="font-size:13px">RPC</a> <a href="/tags/Raspberry-Pi/" style="font-size:13.33px">Raspberry Pi</a> <a href="/tags/Redis/" style="font-size:13.33px">Redis</a> <a href="/tags/Scala/" style="font-size:13.33px">Scala</a> <a href="/tags/Selenium/" style="font-size:13px">Selenium</a> <a href="/tags/Shiro/" style="font-size:13px">Shiro</a> <a href="/tags/Spark/" style="font-size:13.67px">Spark</a> <a href="/tags/Spring/" style="font-size:13.33px">Spring</a> <a href="/tags/Storm/" style="font-size:13.33px">Storm</a> <a href="/tags/Struts2/" style="font-size:13px">Struts2</a> <a href="/tags/Ubuntu/" style="font-size:13.83px">Ubuntu</a> <a href="/tags/Vue/" style="font-size:13px">Vue</a> <a href="/tags/WebSocket/" style="font-size:13.17px">WebSocket</a> <a href="/tags/Wow/" style="font-size:13px">Wow</a> <a href="/tags/Zookeeper/" style="font-size:13px">Zookeeper</a> <a href="/tags/ceph/" style="font-size:13px">ceph</a> <a href="/tags/hive/" style="font-size:13px">hive</a> <a href="/tags/k-means/" style="font-size:13px">k-means</a> <a href="/tags/kafka/" style="font-size:13.17px">kafka</a> <a href="/tags/zTree/" style="font-size:13px">zTree</a> <a href="/tags/图/" style="font-size:13.67px">图</a> <a href="/tags/递归/" style="font-size:13.17px">递归</a> <a href="/tags/问题总结/" style="font-size:14px">问题总结</a></div></div><div class="widget"><h3 class="widget-title">Archive</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a><span class="archive-list-count">22</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">5</span></li></ul></div></div><div class="widget"><h3 class="widget-title">Recent Posts</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/算法与数据结构/">算法与数据结构</a></p><p class="item-title"><a href="/2020/11/26/2020年11月24日22:58:10_邻接表有向图/" class="title">邻接表有向图</a></p><p class="item-date"><time datetime="2020-11-26T12:30:41.025Z" itemprop="datePublished">2020-11-26</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/算法与数据结构/">算法与数据结构</a></p><p class="item-title"><a href="/2020/11/26/2020年11月22日14:11:18_邻接矩阵有向图/" class="title">邻接矩阵有向图</a></p><p class="item-date"><time datetime="2020-11-26T12:30:41.025Z" itemprop="datePublished">2020-11-26</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/算法与数据结构/">算法与数据结构</a></p><p class="item-title"><a href="/2020/11/26/2020年11月21日23:02:20_深度优先遍历算法/" class="title">深度优先遍历算法</a></p><p class="item-date"><time datetime="2020-11-26T12:30:41.025Z" itemprop="datePublished">2020-11-26</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/其他/">其他</a></p><p class="item-title"><a href="/2020/11/26/2020年11月15日14:11:53_使用docker部署hadoop/" class="title">Docker部署hadoop</a></p><p class="item-date"><time datetime="2020-11-26T12:30:41.024Z" itemprop="datePublished">2020-11-26</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/其他/">其他</a></p><p class="item-title"><a href="/2020/11/26/2020年11月15日13:38:15_高效VIM编辑器/" class="title">高效VIM编辑器</a></p><p class="item-date"><time datetime="2020-11-26T12:30:41.024Z" itemprop="datePublished">2020-11-26</time></p></div></li></ul></div></div></div></aside><main class="main" role="main"><div class="content"><article id="post-2019–11-07-KafkaProducer源码刨析" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">Kafka源码刨析之Produer发送模型（一）</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/2020/11/26/2019–11-07-KafkaProducer源码刨析/" class="article-date"><time datetime="2020-11-26T12:30:41.018Z" itemprop="datePublished">2020-11-26</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/大数据/">大数据</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/kafka/">kafka</a></span> <span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span></span></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2020/11/26/2019–11-07-KafkaProducer源码刨析/#comments" class="article-comment-link">Comments</a></span></div></div><div class="article-entry marked-body" itemprop="articleBody"><p>kafka是一个分布式的消息中间件，目前应用十分广泛。看源码不仅可以了解其底层的细节，同时，在看代码时，也能跟着大神们学到很多的编程技巧。</p><h3 id="kafkaproducer的使用"><a class="markdownIt-Anchor" href="#kafkaproducer的使用"></a> KafkaProducer的使用</h3><p>在Kafka中，Client端是由Java实现的，Server端是Scala实现的。下面我们从Client端开始，分析一下Kafaka中的Producer模型。开始之前我们先看一下怎么向Topic中生产数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line">import org.apache.kafka.clients.producer.Producer;</span><br><span class="line"></span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Created by matt on 16/7/26.</span><br><span class="line"> */</span><br><span class="line">public class ProducerTest &#123;</span><br><span class="line">    private static String topicName;</span><br><span class="line">    private static int msgNum;</span><br><span class="line">    private static int key;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092,127.0.0.2:9092&quot;);</span><br><span class="line">        props.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        props.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        topicName = &quot;test&quot;;</span><br><span class="line">        msgNum = 10; // 发送的消息数</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line">        for (int i = 0; i &lt; msgNum; i++) &#123;</span><br><span class="line">            String msg = i + &quot; This is matt&apos;s blog.&quot;;</span><br><span class="line">            producer.send(new ProducerRecord&lt;String, String&gt;(topicName, msg));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面可以看到如何向Topic中生产数据，Kafka在这方面封装的很好，只需要两步就可以完成操作：</p><pre><code>1. 初始化KafkaProducer类
2. 调用send接口发送数据
</code></pre><p>下面围绕着send接口开始展开。</p><h3 id="kafkaproducer中的send方法"><a class="markdownIt-Anchor" href="#kafkaproducer中的send方法"></a> KafkaProducer中的send方法</h3><p>用户使用producer.send发送数据，我们看一下send()的实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 异步向一个 topic 发送数据</span><br><span class="line">@Override</span><br><span class="line">public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record) &#123;</span><br><span class="line">    return send(record, null);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 向 topic 异步地发送数据，当发送确认后唤起回调函数</span><br><span class="line">@Override</span><br><span class="line">public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123;</span><br><span class="line">    // intercept the record, which can be potentially modified; this method does not throw exceptions</span><br><span class="line">    ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors == null ? record : this.interceptors.onSend(record);</span><br><span class="line">    return doSend(interceptedRecord, callback);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接口最后会走一个doSend()方法，接着追进去</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123;</span><br><span class="line">       TopicPartition tp = null;</span><br><span class="line">       try &#123;</span><br><span class="line">           // 1.确认数据要发送到的 topic 的 metadata 是可用的</span><br><span class="line">           ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);</span><br><span class="line">           long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);</span><br><span class="line">           Cluster cluster = clusterAndWaitTime.cluster;</span><br><span class="line">           // 2.序列化 record 的 key 和 value</span><br><span class="line">           byte[] serializedKey;</span><br><span class="line">           try &#123;</span><br><span class="line">               serializedKey = keySerializer.serialize(record.topic(), record.key());</span><br><span class="line">           &#125; catch (ClassCastException cce) &#123;</span><br><span class="line">               throw new SerializationException(&quot;Can&apos;t convert key of class &quot; + record.key().getClass().getName() +</span><br><span class="line">                       &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                       &quot; specified in key.serializer&quot;);</span><br><span class="line">           &#125;</span><br><span class="line">           byte[] serializedValue;</span><br><span class="line">           try &#123;</span><br><span class="line">               serializedValue = valueSerializer.serialize(record.topic(), record.value());</span><br><span class="line">           &#125; catch (ClassCastException cce) &#123;</span><br><span class="line">               throw new SerializationException(&quot;Can&apos;t convert value of class &quot; + record.value().getClass().getName() +</span><br><span class="line">                       &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                       &quot; specified in value.serializer&quot;);</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           // 3. 获取该 record 的 partition 的值（可以指定,也可以根据算法计算）</span><br><span class="line">           int partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">           int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);</span><br><span class="line">           ensureValidRecordSize(serializedSize); // record 的字节超出限制或大于内存限制时,就会抛出 RecordTooLargeException 异常</span><br><span class="line">           tp = new TopicPartition(record.topic(), partition);</span><br><span class="line">           long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp(); // 时间戳</span><br><span class="line">           log.trace(&quot;Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;&quot;, record, callback, record.topic(), partition);</span><br><span class="line">           Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp);</span><br><span class="line">           // 4. 向 accumulator 中追加数据</span><br><span class="line">           RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue,interceptCallback, remainingWaitMs);</span><br><span class="line">           // 5. 如果 batch 已经满了,唤醒 sender 线程发送数据</span><br><span class="line">           if (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">               log.trace(&quot;Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch&quot;,record.topic(),partition);</span><br><span class="line">               this.sender.wakeup();</span><br><span class="line">           &#125;</span><br><span class="line">           return result.future;</span><br><span class="line">       &#125; catch (ApiException e) &#123;</span><br><span class="line">           log.debug(&quot;Exception occurred during message send:&quot;, e);</span><br><span class="line">           if (callback != null)</span><br><span class="line">               callback.onCompletion(null, e);</span><br><span class="line">           this.errors.record();</span><br><span class="line">           if (this.interceptors != null)</span><br><span class="line">               this.interceptors.onSendError(record, tp, e);</span><br><span class="line">           return new FutureFailure(e);</span><br><span class="line">       &#125; catch (InterruptedException e) &#123;</span><br><span class="line">           this.errors.record();</span><br><span class="line">           if (this.i nterceptors != null)</span><br><span class="line">               this.interceptors.onSendError(record, tp, e);</span><br><span class="line">           throw new InterruptException(e);</span><br><span class="line">       &#125; catch (BufferExhaustedException e) &#123;</span><br><span class="line">           this.errors.record();</span><br><span class="line">           this.metrics.sensor(&quot;buffer-exhausted-records&quot;).record();</span><br><span class="line">           if (this.interceptors != null)</span><br><span class="line">               this.interceptors.onSendError(record, tp, e);</span><br><span class="line">           throw e;</span><br><span class="line">       &#125; catch (KafkaException e) &#123;</span><br><span class="line">           this.errors.record();</span><br><span class="line">           if (this.interceptors != null)</span><br><span class="line">               this.interceptors.onSendError(record, tp, e);</span><br><span class="line">           throw e;</span><br><span class="line">       &#125; catch (Exception e) &#123;</span><br><span class="line">           if (this.interceptors != null)</span><br><span class="line">               this.interceptors.onSendError(record, tp, e);</span><br><span class="line">           throw e;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 dosend() 方法的实现上，一条 Record 数据的发送，可以分为以下五步：</p><pre><code>1. 确认数据要发送到的 topic 的 metadata 是可用的（如果该 partition 的 leader 存在则是可用的，如果开启权限时，client 有相应的权限），如果没有 topic 的 metadata 信息，就需要获取相应的 metadata；
2. 序列化 record 的 key 和 value；
3. 获取该 record 要发送到的 partition（可以指定，也可以根据算法计算）；
4. 向 accumulator 中追加 record 数据，数据会先进行缓存；
5. 如果追加完数据后，对应的 RecordBatch 已经达到了 batch.size 的大小（或者batch 的剩余空间不足以添加下一条 Record），则唤醒 sender 线程发送数据。
</code></pre><p>数据的发送过程，可以简单总结为以上五点，下面会这几部分的具体实现进行详细分析。</p><h2 id="发送的过程详解"><a class="markdownIt-Anchor" href="#发送的过程详解"></a> 发送的过程详解</h2><h3 id="获取-topic-的-metadata-信息"><a class="markdownIt-Anchor" href="#获取-topic-的-metadata-信息"></a> 获取 topic 的 metadata 信息</h3><p>Producer 通过 waitOnMetadata() 方法来获取对应 topic 的 metadata 信息，这部分后面会单独抽出一篇文章来介绍，这里就不再详述，总结起来就是：在数据发送前，需要先该 topic 是可用的。</p><h3 id="key-和-value-的序列化"><a class="markdownIt-Anchor" href="#key-和-value-的序列化"></a> key 和 value 的序列化</h3><p>Producer 端对 record 的 key 和 value 值进行序列化操作，在 Consumer 端再进行相应的反序列化，Kafka 内部提供的序列化和反序列化算法如下图所示：</p><p><img src="https://i.loli.net/2019/11/07/r4CtIEAcmhwN7xo.jpg" alt="D8B532DC-1C96-4778-875F-2466E5835EFA.jpeg"><br>Need support? Please accept cookies and refresh the page 😃</p><p>Kafka serialize &amp; deserialize</p><p>当然我们也是可以自定义序列化的具体实现，不过一般情况下，Kafka 内部提供的这些方法已经足够使用。</p><h3 id="获取-partition-值"><a class="markdownIt-Anchor" href="#获取-partition-值"></a> 获取 partition 值</h3><p>关于 partition 值的计算，分为三种情况：</p><p>指明 partition 的情况下，直接将指明的值直接作为 partiton 值；<br>没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；<br>既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。<br>具体实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 当 record 中有 partition 值时，直接返回，没有的情况下调用 partitioner 的类的 partition 方法去计算（KafkaProducer.class）</span><br><span class="line">private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) &#123;</span><br><span class="line">    Integer partition = record.partition();</span><br><span class="line">    return partition != null ?</span><br><span class="line">            partition :</span><br><span class="line">            partitioner.partition(</span><br><span class="line">                    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Producer 默认使用的<code>partitioner</code>是<code>org.apache.kafka.clients.producer.internals.DefaultPartitioner</code>，用户也可以自定义 partition 的策略，下面是这个类两个方法的具体实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        int numPartitions = partitions.size();</span><br><span class="line">        if (keyBytes == null) &#123;// 没有指定 key 的情况下</span><br><span class="line">            int nextValue = nextValue(topic); // 第一次的时候产生一个随机整数,后面每次调用在之前的基础上自增;</span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            // leader 不为 null,即为可用的 partition</span><br><span class="line">            if (availablePartitions.size() &gt; 0) &#123;</span><br><span class="line">                int part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                return availablePartitions.get(part).partition();</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                return Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else &#123;// 有 key 的情况下,使用 key 的 hash 值进行计算</span><br><span class="line">            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; // 选择 key 的 hash 值</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 根据 topic 获取对应的整数变量</span><br><span class="line">    private int nextValue(String topic) &#123;</span><br><span class="line">        AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">        if (null == counter) &#123; // 第一次调用时，随机产生</span><br><span class="line">            counter = new AtomicInteger(new Random().nextInt());</span><br><span class="line">            AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">            if (currentCounter != null) &#123;</span><br><span class="line">                counter = currentCounter;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return counter.getAndIncrement(); // 后面再调用时，根据之前的结果自增</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这就是 Producer 中默认的 partitioner 实现。</p><h3 id="向-accumulator-写数据"><a class="markdownIt-Anchor" href="#向-accumulator-写数据"></a> 向 accumulator 写数据</h3><p>Producer 会先将 record 写入到 buffer 中，当达到一个 batch.size 的大小时，再唤起 sender 线程去发送 RecordBatch（第五步），这里先详细分析一下 Producer 是如何向 buffer 中写入数据的。</p><p>Producer 是通过 RecordAccumulator 实例追加数据，RecordAccumulator 模型如下图所示，一个重要的变量就是 ConcurrentMap&lt;TopicPartition, Deque<recordbatch>&gt; batches，每个 TopicPartition 都会对应一个 Deque<recordbatch>，当添加数据时，会向其 topic-partition 对应的这个 queue 最新创建的一个 RecordBatch 中添加 record，而发送数据时，则会先从 queue 中最老的那个 RecordBatch 开始发送。</recordbatch></recordbatch></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![C89D1394-08D6-416D-814D-BC19D65091B2.jpeg](https://i.loli.net/2019/11/07/C6bS5ieDGfnAOsg.jpg)</span><br></pre></td></tr></table></figure><p>Producer RecordAccumulator 模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">// org.apache.kafka.clients.producer.internals.RecordAccumulator</span><br><span class="line">     // 向 accumulator 添加一条 record，并返回添加后的结果（结果主要包含: future metadata、batch 是否满的标志以及新 batch 是否创建）其中， maxTimeToBlock 是 buffer.memory 的 block 的最大时间</span><br><span class="line">    public RecordAppendResult append(TopicPartition tp,</span><br><span class="line">                                     long timestamp,</span><br><span class="line">                                     byte[] key,</span><br><span class="line">                                     byte[] value,</span><br><span class="line">                                     Callback callback,</span><br><span class="line">                                     long maxTimeToBlock) throws InterruptedException &#123;</span><br><span class="line">        appendsInProgress.incrementAndGet();</span><br><span class="line">        try &#123;</span><br><span class="line">            Deque&lt;RecordBatch&gt; dq = getOrCreateDeque(tp);// 每个 topicPartition 对应一个 queue</span><br><span class="line">            synchronized (dq) &#123;// 在对一个 queue 进行操作时,会保证线程安全</span><br><span class="line">                if (closed)</span><br><span class="line">                    throw new IllegalStateException(&quot;Cannot send after the producer is closed.&quot;);</span><br><span class="line">                RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq); // 追加数据</span><br><span class="line">                if (appendResult != null)// 这个 topic-partition 已经有记录了</span><br><span class="line">                    return appendResult;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            // 为 topic-partition 创建一个新的 RecordBatch, 需要初始化相应的 RecordBatch，要为其分配的大小是: max（batch.size, 加上头文件的本条消息的大小）</span><br><span class="line">            int size = Math.max(this.batchSize, Records.LOG_OVERHEAD + Record.recordSize(key, value));</span><br><span class="line">            log.trace(&quot;Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;&quot;, size, tp.topic(), tp.partition());</span><br><span class="line">            ByteBuffer buffer = free.allocate(size, maxTimeToBlock);// 给这个 RecordBatch 初始化一个 buffer</span><br><span class="line">            synchronized (dq) &#123;</span><br><span class="line">                if (closed)</span><br><span class="line">                    throw new IllegalStateException(&quot;Cannot send after the producer is closed.&quot;);</span><br><span class="line"></span><br><span class="line">                RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq);</span><br><span class="line">                if (appendResult != null) &#123;// 如果突然发现这个 queue 已经存在，那么就释放这个已经分配的空间</span><br><span class="line">                    free.deallocate(buffer);</span><br><span class="line">                    return appendResult;</span><br><span class="line">                &#125;</span><br><span class="line">                // 给 topic-partition 创建一个 RecordBatch</span><br><span class="line">                MemoryRecordsBuilder recordsBuilder = MemoryRecords.builder(buffer, compression, TimestampType.CREATE_TIME, this.batchSize);</span><br><span class="line">                RecordBatch batch = new RecordBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line">                // 向新的 RecordBatch 中追加数据</span><br><span class="line">                FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, callback, time.milliseconds()));</span><br><span class="line"></span><br><span class="line">                dq.addLast(batch);// 将 RecordBatch 添加到对应的 queue 中</span><br><span class="line">                incomplete.add(batch);// 向未 ack 的 batch 集合添加这个 batch</span><br><span class="line">                // 如果 dp.size()&gt;1 就证明这个 queue 有一个 batch 是可以发送了</span><br><span class="line">                return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            appendsInProgress.decrementAndGet();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>总结一下其 record 写入的具体流程如下图所示：</p><p><img src="https://i.loli.net/2019/11/07/bL3CzBcH7G1f2Pe.jpg" alt="2EC40DD6-2B51-4F00-B2C7-CB6242E47B8F.jpeg"></p><p>Producer RecordAccumulator record 写入流程</p><p>获取该 topic-partition 对应的 queue，没有的话会创建一个空的 queue；<br>向 queue 中追加数据，先获取 queue 中最新加入的那个 RecordBatch，如果不存在或者存在但剩余空余不足以添加本条 record 则返回 null，成功写入的话直接返回结果，写入成功；<br>创建一个新的 RecordBatch，初始化内存大小根据 max(batch.size, Records.LOG_OVERHEAD + Record.recordSize(key, value)) 来确定（防止单条 record 过大的情况）；<br>向新建的 RecordBatch 写入 record，并将 RecordBatch 添加到 queue 中，返回结果，写入成功。</p><h3 id="发送-recordbatch"><a class="markdownIt-Anchor" href="#发送-recordbatch"></a> 发送 RecordBatch</h3><p>当 record 写入成功后，如果发现 RecordBatch 已满足发送的条件（通常是 queue 中有多个 batch，那么最先添加的那些 batch 肯定是可以发送了），那么就会唤醒 sender 线程，发送 RecordBatch。</p><p>sender 线程对 RecordBatch 的处理是在 run() 方法中进行的，该方法具体实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">void run(long now) &#123;</span><br><span class="line">        Cluster cluster = metadata.fetch();</span><br><span class="line">        // 获取那些已经可以发送的 RecordBatch 对应的 nodes</span><br><span class="line">        RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);</span><br><span class="line"></span><br><span class="line">        // 如果有 topic-partition 的 leader 是未知的,就强制 metadata 更新</span><br><span class="line">        if (!result.unknownLeaderTopics.isEmpty()) &#123;</span><br><span class="line">            for (String topic : result.unknownLeaderTopics)</span><br><span class="line">                this.metadata.add(topic);</span><br><span class="line">            this.metadata.requestUpdate();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 如果与node 没有连接（如果可以连接,同时初始化该连接）,就证明该 node 暂时不能发送数据,暂时移除该 node</span><br><span class="line">        Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class="line">        long notReadyTimeout = Long.MAX_VALUE;</span><br><span class="line">        while (iter.hasNext()) &#123;</span><br><span class="line">            Node node = iter.next();</span><br><span class="line">            if (!this.client.ready(node, now)) &#123;</span><br><span class="line">                iter.remove();</span><br><span class="line">                notReadyTimeout = Math.min(notReadyTimeout, this.client.connectionDelay(node, now));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 返回该 node 对应的所有可以发送的 RecordBatch 组成的 batches（key 是 node.id）,并将 RecordBatch 从对应的 queue 中移除</span><br><span class="line">        Map&lt;Integer, List&lt;RecordBatch&gt;&gt; batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);</span><br><span class="line">        if (guaranteeMessageOrder) &#123;</span><br><span class="line">            //记录将要发送的 RecordBatch</span><br><span class="line">            for (List&lt;RecordBatch&gt; batchList : batches.values()) &#123;</span><br><span class="line">                for (RecordBatch batch : batchList)</span><br><span class="line">                    this.accumulator.mutePartition(batch.topicPartition);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 将由于元数据不可用而导致发送超时的 RecordBatch 移除</span><br><span class="line">        List&lt;RecordBatch&gt; expiredBatches = this.accumulator.abortExpiredBatches(this.requestTimeout, now);</span><br><span class="line">        for (RecordBatch expiredBatch : expiredBatches)</span><br><span class="line">            this.sensors.recordErrors(expiredBatch.topicPartition.topic(), expiredBatch.recordCount);</span><br><span class="line"></span><br><span class="line">        sensors.updateProduceRequestMetrics(batches);</span><br><span class="line"></span><br><span class="line">        long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);</span><br><span class="line">        if (!result.readyNodes.isEmpty()) &#123;</span><br><span class="line">            log.trace(&quot;Nodes with data ready to send: &#123;&#125;&quot;, result.readyNodes);</span><br><span class="line">            pollTimeout = 0;</span><br><span class="line">        &#125;</span><br><span class="line">        // 发送 RecordBatch</span><br><span class="line">        sendProduceRequests(batches, now);</span><br><span class="line"></span><br><span class="line">        this.client.poll(pollTimeout, now); // 关于 socket 的一些实际的读写操作（其中包括 meta 信息的更新）</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这段代码前面有很多是其他的逻辑处理，如：移除暂时不可用的 node、处理由于元数据不可用导致的超时RecordBatch，真正进行发送发送RecordBatch的是sendProduceRequests(batches, now)这个方法，具体是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Transfer the record batches into a list of produce requests on a per-node basis</span><br><span class="line"> */</span><br><span class="line">private void sendProduceRequests(Map&lt;Integer, List&lt;RecordBatch&gt;&gt; collated, long now) &#123;</span><br><span class="line">    for (Map.Entry&lt;Integer, List&lt;RecordBatch&gt;&gt; entry : collated.entrySet())</span><br><span class="line">        sendProduceRequest(now, entry.getKey(), acks, requestTimeout, entry.getValue());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Create a produce request from the given record batches</span><br><span class="line"> */</span><br><span class="line">// 发送 produce 请求</span><br><span class="line">private void sendProduceRequest(long now, int destination, short acks, int timeout, List&lt;RecordBatch&gt; batches) &#123;</span><br><span class="line">    Map&lt;TopicPartition, MemoryRecords&gt; produceRecordsByPartition = new HashMap&lt;&gt;(batches.size());</span><br><span class="line">    final Map&lt;TopicPartition, RecordBatch&gt; recordsByPartition = new HashMap&lt;&gt;(batches.size());</span><br><span class="line">    for (RecordBatch batch : batches) &#123;</span><br><span class="line">        TopicPartition tp = batch.topicPartition;</span><br><span class="line">        produceRecordsByPartition.put(tp, batch.records());</span><br><span class="line">        recordsByPartition.put(tp, batch);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ProduceRequest.Builder requestBuilder =</span><br><span class="line">            new ProduceRequest.Builder(acks, timeout, produceRecordsByPartition);</span><br><span class="line">    RequestCompletionHandler callback = new RequestCompletionHandler() &#123;</span><br><span class="line">        public void onComplete(ClientResponse response) &#123;</span><br><span class="line">            handleProduceResponse(response, recordsByPartition, time.milliseconds());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    String nodeId = Integer.toString(destination);</span><br><span class="line">    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, callback);</span><br><span class="line">    client.send(clientRequest, now);</span><br><span class="line">    log.trace(&quot;Sent produce request to &#123;&#125;: &#123;&#125;&quot;, nodeId, requestBuilder);</span><br></pre></td></tr></table></figure><p>这段代码就简单很多，总来起来就是，将 batches 中 leader 为同一个 node 的所有 RecordBatch 放在一个请求中进行发送。</p><h3 id="最后"><a class="markdownIt-Anchor" href="#最后"></a> 最后</h3><p>本文是对 Kafka Producer 端发送模型的一个简单分析，下一篇文章将会详细介绍 metadata 相关的内容，包括 metadata 的内容以及在 Producer 端 metadata 的更新机制。</p><p>转自：<a href="https://zhuanlan.zhihu.com/p/66190242" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/66190242</a></p></div><div class="article-footer"><blockquote class="mt-2x"></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/Demo233" target="_blank" class="img-burn thumb-sm visible-lg"><img src="/images/avatar.jpg" class="img-rounded w-full" alt=""></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/Demo233" target="_blank"><span class="text-dark">zyh</span><small class="ml-1x"></small></a></h3><div>个人简介。</div></div></figure></div></div></div></article><section id="comments"></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/2020/11/26/2019-11-07-Kafka源码刨析之Produer Metadata更新机制（二）/" title="Kafka源码刨析之Produer Metadata更新机制（二）"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;Newer</span></a></li><li class="next"><a href="/2019/08/20/2019–08-20-神经网络：基本分类[转]/" title="神经网络：基本分类[转]"><span>Older&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li></ul><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/Demo233" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="https://www.weibo.com/u/5592904662/home" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top"><i class="icon icon-weibo"></i></a></li><li><a href="https://twitter.com/cnnqjban521" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top"><i class="icon icon-twitter"></i></a></li><li><a href="https://www.facebook.com/yihao.zhao.7" target="_blank" title="Facebook" data-toggle="tooltip" data-placement="top"><i class="icon icon-facebook"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>!function(T){var n={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)"},ROOT_URL:"/",CONTENT_URL:"/content.json"};T.INSIGHT_CONFIG=n}(window)</script><script src="/js/insight.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn.jsdelivr.net/npm/gitalk@1.4.0/dist/gitalk.min.js"></script><script src="//cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"b6eff4e9fc3dae3c8764",clientSecret:"1d0201dabf2358e2f833a3b2e51b99126d85045a",repo:"Demo233.github.io",owner:"Demo233",admin:["Demo233"],id:md5(location.pathname),distractionFreeMode:!0});gitalk.render("comments")</script></body></html><!-- rebuild by neat -->